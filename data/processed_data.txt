df14f9efd761449067f9ba6e8d2bdf251ff579ec from logger import * from ua_constants import * import tempfile import xml.dom.minidom as dom import os import string from collections import Counter from ua_namespace import opcua_node_id_t class preProcessDocument:   originXML = ''    targetXML = ()    nodeset   = ''    parseOK   = False;   containedNodes  = []    referencedNodes = []    namespaceOrder  = []    namespaceQualifiers = []         referencedNamesSpaceUris = []       def __init__(self, originXML):     self.originXML = originXML     self.targetXML = tempfile.mkstemp(prefix=os.path.basename(originXML)+"_preProcessed-" ,suffix=".xml")     self.parseOK   = True     self.containedNodes  = []     self.referencedNodes = []     self.namespaceOrder  = []     self.referencedNamesSpaceUris = []     self.namespaceQualifiers = []     try:       self.nodeset = dom.parse(originXML)       if len(self.nodeset.getElementsByTagName("UANodeSet")) == 0 or len(self.nodeset.getElementsByTagName("UANodeSet")) > 1:         log(self, "Document " + self.targetXML[1] + " contains no or more then 1 nodeset", LOG_LEVEL_ERROR)         self.parseOK   = False     except:       self.parseOK   = False     log(self, "Adding new document to be preprocessed " + os.path.basename(originXML) + " as " + self.targetXML[1], LOG_LEVEL_DEBUG)      def clean(self):          os.remove(self.targetXML[1])      def getTargetXMLName(self):     if (self.parseOK):       return self.targetXML[1]     return None      def extractNamespaceURIs(self):     """ extractNamespaceURIs                  minidom gobbles up <NamespaceUris></NamespaceUris> elements, without a decent         way to reliably access this dom2 <uri></uri> elements (only attribute xmlns= are          accessible using minidom).  We need them for dereferencing though... This          function attempts to do just that.                  returns: Nothing     """     infile = open(self.originXML)     foundURIs = False     nsline = ""     line = infile.readline()     for line in infile:       if "<namespaceuris>" in line.lower():         foundURIs = True       elif "</namespaceuris>" in line.lower():         foundURIs = False         nsline = nsline + line         break       if foundURIs:         nsline = nsline + line          if len(nsline) > 0:       ns = dom.parseString(nsline).getElementsByTagName("NamespaceUris")       for uri in ns[0].childNodes:         if uri.nodeType != uri.ELEMENT_NODE:           continue         self.referencedNamesSpaceUris.append(uri.firstChild.data)            infile.close()        def analyze(self):     """ analyze()              analyze will gather information about the nodes and references contained in a XML File         to facilitate later preprocessing stages that adresss XML dependency issues                  returns: No return value     """      nodeIds = []     ns = self.nodeset.getElementsByTagName("UANodeSet")                    self.extractNamespaceURIs()           for key in ns[0].attributes.keys():        if "xmlns:" in key:           self.namespaceQualifiers.append(key.replace("xmlns:",""))       if "xmlns:s" in key:          self.namespaceOrder.append((int(key.replace("xmlns:s","")), ns[0].getAttribute(key)))               for nd in ns[0].childNodes:       if nd.nodeType != nd.ELEMENT_NODE:         continue       if nd.hasAttribute(u'NodeId'):         self.containedNodes.append( (opcua_node_id_t(nd.getAttribute(u'NodeId')), nd) )         refs = nd.getElementsByTagName(u'References')[0]         for ref in refs.childNodes:           if ref.nodeType == ref.ELEMENT_NODE:             self.referencedNodes.append( (opcua_node_id_t(ref.firstChild.data), ref) )          log(self, "Nodes: " + str(len(self.containedNodes)) + " References: " + str(len(self.referencedNodes)), LOG_LEVEL_DEBUG)      def getNamespaceId(self):     """ namespaceId()                  Counts the namespace IDs in all nodes of this XML and picks the most used         namespace as the numeric identifier of this data model.                  returns: Integer ID of the most propable/most used namespace in this XML     """     max = 0;     namespaceIdGuessed = 0;     idDict = {}          for ndid in self.containedNodes:       if not idDict.has_key(ndid[0].ns):         idDict[ndid[0].ns] = 1       else:         idDict[ndid[0].ns] = idDict[ndid[0].ns] + 1          for entry in idDict:       if idDict[entry] > max:         max = idDict[entry]         namespaceIdGuessed = entry     log(self, "XML Contents are propably in namespace " + str(entry) + " (used by " + str(idDict[entry]) + " Nodes)", LOG_LEVEL_DEBUG)     return namespaceIdGuessed      def getReferencedNamespaceUri(self, nsId):     """ getReferencedNamespaceUri              returns an URL that hopefully corresponds to the nsId that was used to reference this model                  return: URI string corresponding to nsId     """          if len(self.namespaceOrder) > 0:       for el in self.namespaceOrder:         if el[0] == nsId:           return el[1]                    if len(self.referencedNamesSpaceUris) > 0  and len(self.referencedNamesSpaceUris) >= nsId-1:       return self.referencedNamesSpaceUris[nsId-1]               return ""      def getNamespaceDependencies(self):     deps = []     for ndid in self.referencedNodes:       if not ndid[0].ns in deps:         deps.append(ndid[0].ns)     return deps        def finalize(self):     outfile = self.targetXML[0]     outline = self.nodeset.toxml()     for qualifier in self.namespaceQualifiers:       rq = qualifier+":"       outline = outline.replace(rq.decode('UTF-8'), "")     os.write(outfile, outline.encode('UTF-8'))     os.close(outfile)        def reassignReferencedNamespaceId(self, currentNsId, newNsId):     """ reassignReferencedNamespaceId                  Iterates over all references in this document, find references to currentNsId and changes them to newNsId.         NodeIds themselves are not altered.                  returns: nothing     """      for refNd in self.referencedNodes:       if refNd[0].ns == currentNsId:         refNd[1].firstChild.data = refNd[1].firstChild.data.replace("ns="+str(currentNsId), "ns="+str(newNsId))         refNd[0].ns = newNsId         refNd[0].toString()      def reassignNamespaceId(self, currentNsId, newNsId):     """ reassignNamespaceId                  Iterates over all nodes in this document, find those in namespace currentNsId and changes them to newNsId.                  returns: nothing     """      log(self, "Migrating nodes /w ns index " + str(currentNsId) + " to " + str(newNsId), LOG_LEVEL_DEBUG)     for nd in self.containedNodes:       if nd[0].ns == currentNsId:                  for refNd in self.referencedNodes:           if refNd[0].ns == currentNsId and refNd[0] == nd[0]:             refNd[1].firstChild.data = refNd[1].firstChild.data.replace("ns="+str(currentNsId), "ns="+str(newNsId))             refNd[0].ns = newNsId             refNd[0].toString()         nd[1].setAttribute(u'NodeId', nd[1].getAttribute(u'NodeId').replace("ns="+str(currentNsId), "ns="+str(newNsId)))         nd[0].ns = newNsId         nd[0].toString()    class open62541_XMLPreprocessor:   preProcDocuments = []      def __init__(self):       self.preProcDocuments = []          def addDocument(self, documentPath):     self.preProcDocuments.append(preProcessDocument(documentPath))        def removePreprocessedFiles(self):     for doc in self.preProcDocuments:       doc.clean()      def getPreProcessedFiles(self):     files = []     for doc in self.preProcDocuments:       if (doc.parseOK):         files.append(doc.getTargetXMLName())     return files      def testModelCongruencyAgainstReferences(self, doc, refs):     """ testModelCongruencyAgainstReferences              Counts how many of the nodes referencef in refs can be found in the model         doc.                  returns: double corresponding to the percentage of hits     """     sspace = len(refs)     if sspace == 0:       return float(0)     found   = 0     for ref in refs:       for n in doc.containedNodes:         if str(ref) == str(n[0]):           print ref, n[0]           found = found + 1           break     return float(found)/float(sspace)        def preprocess_assignUniqueNsIds(self):     nsdep  = []     docLst = []               for doc in self.preProcDocuments:       if doc.getNamespaceId() == 0:         docLst.append(doc)     for doc in docLst:       self.preProcDocuments.remove(doc)               nsidx = 1      for doc in self.preProcDocuments:       nsidx = nsidx + 1       nsid = doc.getNamespaceId()       doc.reassignNamespaceId(nsid, nsidx)       docLst.append(doc)       log(self, "Document " + doc.originXML + " is now namespace " + str(nsidx), LOG_LEVEL_INFO)     self.preProcDocuments = docLst      def getUsedNamespaceArrayNames(self):     """ getUsedNamespaceArrayNames              Returns the XML xmlns:s1 or <URI>[0] of each XML document (if contained/possible)                  returns: dict of int:nsId -> string:url     """     nsName = {}     for doc in self.preProcDocuments:       uri = doc.getReferencedNamespaceUri(1)       if uri == None:         uri = "http://modeluri.not/retrievable/from/xml"       nsName[doc.getNamespaceId()] = doc.getReferencedNamespaceUri(1)     return nsName          def preprocess_linkDependantModels(self):         revertToStochastic = []                for doc in self.preProcDocuments:       nsid = doc.getNamespaceId()       dependencies = doc.getNamespaceDependencies()       for d in dependencies:         if d != nsid and d != 0:                      nsUri = doc.getReferencedNamespaceUri(d)            log(self, "Need a namespace referenced as " + str(d) + ". Which hopefully is " + nsUri, LOG_LEVEL_INFO)           targetDoc = None           for tgt in self.preProcDocuments:                                       if tgt.getReferencedNamespaceUri(1) == nsUri:               targetDoc = tgt               break           if not targetDoc == None:                          doc.reassignReferencedNamespaceId(d, targetDoc.getNamespaceId())             continue           else:             revertToStochastic.append((doc, d))              log(self, "Failed to reliably identify which XML/Model " + os.path.basename(doc.originXML) + " calls ns=" +str(d), LOG_LEVEL_WARN)          for (doc, d) in revertToStochastic:       log(self, "Attempting to find stochastic match for target namespace ns=" + str(d) + " of " + os.path.basename(doc.originXML), LOG_LEVEL_WARN)              refs = []       matches = []        for ref in doc.referencedNodes:         if ref[0].ns == d:           refs.append(opcua_node_id_t(str(ref[0])))       for tDoc in self.preProcDocuments:         tDocId = tDoc.getNamespaceId()                  for r in refs:           r.ns = tDocId           r.toString()                  c = self.testModelCongruencyAgainstReferences(tDoc, refs)         print c         if c>0:           matches.append(c, tDoc)       best = (0, None)       for m in matches:         print m[0]         if m[0] > best[0]:           best = m       if best[1] != None:         log(self, "Best match (" + str(best[1]*100) + "%) for what " + os.path.basename(doc.originXML) + " refers to as ns="+str(d)+" was " + os.path.basename(best[1].originXML), LOG_LEVEL_WARN)         doc.reassignReferencedNamespaceId(d, best[1].getNamespaceId())       else:          log(self, "Failed to find a match for what " +  os.path.basename(doc.originXML) + " refers to as ns=" + str(d) ,LOG_LEVEL_ERROR )          def preprocessAll(self):               for doc in self.preProcDocuments:       doc.analyze()                                        self.preprocess_assignUniqueNsIds()     self.preprocess_linkDependantModels()                                             for doc in self.preProcDocuments:       doc.finalize()          return True          
9f8d5ca88193c0b5a4a0c1150de20e4d80e4c5ef """ Core module widgets """ WIDGETS = {'widget_welcome': {'title': 'Quick Start', 'size': "95%"}} def get_widgets(request):     "Returns a set of all available widgets"     return WIDGETS
c22dd5f2ab832bd133103649109fa76ebeb068fa from openerp.osv import orm class account_invoice_line(orm.Model):     _inherit = "account.invoice.line"     def product_id_change(         self, cr, uid, ids, product_id, uom_id, qty=0,         name='', type='out_invoice', partner_id=False, fposition_id=False,         price_unit=False, currency_id=False, context=None, company_id=None     ):         res = super(account_invoice_line, self).product_id_change(             cr, uid, ids, product_id, uom_id, qty=qty,             name=name, type=type, partner_id=partner_id,             fposition_id=fposition_id, price_unit=price_unit,             currency_id=currency_id, context=context, company_id=company_id         )         if product_id:             user = self.pool.get('res.users').browse(                 cr, uid, uid, context=context)             user_groups = [g.id for g in user.groups_id]             ref = self.pool.get('ir.model.data').get_object_reference(                 cr, uid, 'invoice_line_description',                 'group_use_product_description_per_inv_line'             )             if ref and len(ref) > 1 and ref[1]:                 group_id = ref[1]                 if group_id in user_groups:                     product_obj = self.pool.get('product.product')                     product = product_obj.browse(                         cr, uid, product_id, context=context)                     if (                         product                         and product.description                         and 'value' in res                     ):                         res['value']['name'] = product.description         return res
382b3798948be2b389028a8efe4734d90c521d44 from . import stock
0474e23fe4b1dc85858dd93bbed144328a494dd7 KEY_LENGTH = 16 SREG2AX = {          'nickname': 'http://axschema.org/namePerson/friendly',     'email': 'http://axschema.org/contact/email',     'fullname': 'http://axschema.org/namePerson',     'dob': 'http://axschema.org/birthDate',     'gender': 'http://axschema.org/person/gender',     'postcode': 'http://axschema.org/contact/postalCode/home',     'country': 'http://axschema.org/contact/country/home',     'language': 'http://axschema.org/pref/language',     'timezone': 'http://axschema.org/pref/timezone', }
601f16ca544d80fa1102a6c333d082b0beac4e11 from . import test_holidays_flow
181093306ea4658c85970764d3a31286fb4239b8 {     'name': 'OpenUpgrade Records',     'version': '0.2',     'category': 'Normal',     'description': """Allow OpenUpgrade records to be stored in the database and compare with other servers. This module depends on OpenERP client lib:     easy_install openerp-client-lib """,     'author': 'OpenUpgrade Community',     'maintainer': 'OpenUpgrade Community',     'contributors': ['Therp BV'],     'website': 'https://launchpad.net/~openupgrade-committers',     'depends': [],     'init_xml': [],     'update_xml': [         'view/openupgrade_record.xml',         'view/comparison_config.xml',         'view/analysis_wizard.xml',         'view/generate_records_wizard.xml',         'view/install_all_wizard.xml',         'security/ir.model.access.csv',         ],     'demo_xml': [     ],     'test': [     ],     'installable': True,     'auto_install': False,     'external_dependencies': {         'python': ['openerplib'],         }, }
d23ea8d067507ee8890a2bbec4e18b0fb601e349 from openerp.osv import fields,osv class ir_exports(osv.osv):     _name = "ir.exports"     _order = 'name'     _columns = {         'name': fields.char('Export Name'),         'resource': fields.char('Resource', select=True),         'export_fields': fields.one2many('ir.exports.line', 'export_id',                                          'Export ID', copy=True),     } class ir_exports_line(osv.osv):     _name = 'ir.exports.line'     _order = 'id'     _columns = {         'name': fields.char('Field Name'),         'export_id': fields.many2one('ir.exports', 'Export', select=True, ondelete='cascade'),     }
833deb98ceb3f675009f9a820d62746a09849ca6 from openerp.osv import fields, osv class res_company_vat (osv.osv):     _inherit = 'res.company'     _columns = {         'vat_check_vies': fields.boolean('VIES VAT Check',                                          help="If checked, Partners VAT numbers will be fully validated against EU's VIES service "                                               "rather than via a simple format validation (checksum)."),     }     
0e4f949ca97a1ea6fc16c804585ae91a28ae5701 from openerp import fields, models, tools class ReportEventRegistrationQuestions(models.Model):     _name = "event.question.report"     _auto = False     attendee_id = fields.Many2one(comodel_name='event.registration', string='Registration')     question_id = fields.Many2one(comodel_name='event.question', string='Question')     answer_id = fields.Many2one(comodel_name='event.answer', string='Answer')     event_id = fields.Many2one(comodel_name='event.event', string='Event')     def init(self, cr):         """ Event Question main report """         tools.drop_view_if_exists(cr, 'event_question_report')         cr.execute(""" CREATE VIEW event_question_report AS (             SELECT                 att_answer.id as id,                 att_answer.event_registration_id as attendee_id,                 answer.question_id as question_id,                 answer.id as answer_id,                 question.event_id as event_id             FROM                 event_registration_answer as att_answer             LEFT JOIN                 event_answer as answer ON answer.id = att_answer.event_answer_id             LEFT JOIN                 event_question as question ON question.id = answer.question_id             GROUP BY                 attendee_id,                 event_id,                 question_id,                 answer_id,                 att_answer.id         )""")
eabc0730c5d5439fdd45003f765fd23054710dae """ Configuration file for py.test """ import django def pytest_configure():     from django.conf import settings     settings.configure(         DEBUG=True,         USE_TZ=True,         DATABASES={             "default": {                 "ENGINE": "django.db.backends.sqlite3",                 "NAME": "test.sqlite3",             }         },         INSTALLED_APPS=[             "django.contrib.auth",             "django.contrib.contenttypes",             "django.contrib.sites",                                                                 "test_accounts",             "test_vendors",             "organizations",             "test_custom",         ],         MIDDLEWARE_CLASSES=[],         SITE_ID=1,         FIXTURE_DIRS=['tests/fixtures'],         ORGS_SLUGFIELD='autoslug.AutoSlugField',         ROOT_URLCONF="tests.urls",     )     django.setup()
5e53755a8556f460458896160402cab4073559f2 import os.path import mds import mds.date_time import mds.messages import mds.netcdf.copy import time import netcdf_tool import arcpy class OPeNDAPtoNetCDF(netcdf_tool.NetCDFTool):     @staticmethod     def initialize_variables_list(             variables_parameter,             dataset):         """         Initialize *variables_parameter*, given *dataset*.         This function updates the parameter's filter list.         """         if dataset is None:             variables_parameter.filter.list = []         else:             variables_parameter.filter.list = list(                 dataset.data_variable_names())     @staticmethod     def update_variables_list(             variables_parameter,             dataset):         """         Update *variables_parameter*, given *dataset*.         If the currently selected variables are all present in *dataset*, and         only one of them is spatial, the filter is updated to only contain         those data variables that are non-spatial or share the same spatial         dimensions as the spatial variable currently selected. This prevents         the user from selecting incompatible variables.         In all other cases, the current list of selected variables is merged         with the list of variables in *dataset* and used as the filter.         """         assert dataset         selected_variable_names = mds.OrderedSet(variables_parameter.values)         new_variable_names = dataset.data_variable_names()         selected_variables_in_dataset = all([variable_name in new_variable_names             for variable_name in selected_variable_names])         one_spatial_variable_selected = selected_variables_in_dataset and \             len([variable_name for variable_name in selected_variable_names \                 if dataset.is_spatial_variable(variable_name)]) == 1         if selected_variables_in_dataset and one_spatial_variable_selected:             filter_list = list(dataset.compatible_data_variable_names(                 iter(selected_variable_names).next()))             assert len(filter_list) >= 1         else:             unknown_variable_names = selected_variable_names - \                 new_variable_names             filter_list = list(unknown_variable_names | new_variable_names)         variables_parameter.filter.list = filter_list     @staticmethod     def variables_are_compatible(             variable_names,             dataset):         """         Return whether *variable_names* are compatible.         Variables are compatible if they share the same spatial dimensions or         are non-spatial.         """         assert dataset         assert len(variable_names) > 0         assert all([variable_name in dataset.data_variable_names()             for variable_name in variable_names])                  spatial_variable_name = next((variable_name for variable_name in             variable_names if dataset.is_spatial_variable(variable_name)), None)         if spatial_variable_name is not None:                                       compatible_data_variable_names = \                 dataset.compatible_data_variable_names(spatial_variable_name)         else:                                       compatible_data_variable_names = \                 dataset.compatible_data_variable_names(                     iter(variable_names).next())         return all([variable_name in compatible_data_variable_names for             variable_name in variable_names])     @staticmethod     def initialize_output_file(             output_file_parameter,             dataset_parameter):         if dataset_parameter.value is None:             output_file_parameter.value = None         else:             dataset_name = dataset_parameter.valueAsText             filename = "{}.nc".format(os.path.splitext(os.path.basename(                 dataset_name))[0])             output_file_parameter.value = os.path.join(                 OPeNDAPtoNetCDF.default_output_directory_name(dataset_name),                     filename)     @staticmethod     def initialize_extent(             extent_parameter,             dataset,             variables_parameter):         if (dataset is not None) and \                 (len(dataset.spatial_data_variable_names()) == 0):                                       extent_parameter.value = None             extent_parameter.enabled = False         else:                                                    extent_parameter.enabled = True             if dataset is None or variables_parameter.values is None:                 extent_parameter.value = None             else:                 assert len(variables_parameter.values) > 0                                  spatial_variable_name = next((variable_name for variable_name in                     variables_parameter.values if variable_name in                         dataset.data_variable_names() and                             dataset.is_spatial_variable(variable_name)), None)                 if spatial_variable_name is None:                     extent_parameter.value = None                 else:                                                                                    extent = dataset.extent(spatial_variable_name)                     extent_parameter.value = "{} {} {} {}".format(                         extent[0], extent[1], extent[2], extent[3])     @staticmethod     def initialize_dimensions(             dimensions_parameter,             dataset,             value_selection_method,             variables_parameter):         assert value_selection_method in ["BY_VALUE", "BY_INDEX"]         if dataset is None or variables_parameter.values is None:             dimensions_parameter.value = None         else:             assert len(variables_parameter.values) > 0             dimension_names = mds.OrderedSet()             for variable_name in variables_parameter.values:                                                   if variable_name in dataset.variable_names():                     dimension_names |= dataset.variable_dimension_names(                         variable_name)             dimension_names = mds.order(dimension_names,                 dataset.dimension_names())                          spatial_variable_name = next((variable_name for variable_name in                 variables_parameter.values if variable_name in                     dataset.data_variable_names() and                         dataset.is_spatial_variable(variable_name)), None)             if spatial_variable_name is not None:                 space_dimension_names = dataset.space_dimension_names(                     spatial_variable_name)                 dimension_names = dimension_names - space_dimension_names             if len(dimension_names) == 0:                 dimensions_parameter.value = None             else:                                  values = []                 for dimension_name in dimension_names:                     dimension = dataset.dimension(dimension_name)                     if len(dimension) > 0:                         first_index = 0                         last_index = len(dimension) - 1                         start_value = first_index                         end_value = last_index                         if dimension_name in dataset.variable_names() and \                                 value_selection_method == "BY_VALUE":                             dimension_variable = dataset.variable(                                 dimension_name)                             coordinates = dimension_variable[:]                             start_value = coordinates[start_value]                             end_value = coordinates[end_value]                             if len(coordinates.shape) == 1:                                 if dataset.convention.is_time_dimension_variable(                                         dimension_variable):                                     start_value, end_value = \                                         mds.netcdf.coordinates_to_dates([                                             start_value, end_value],                                             dimension_variable)                                     start_value = mds.date_time.to_iso_format(                                         start_value)                                     end_value = mds.date_time.to_iso_format(                                         end_value)                         values.append([dimension_name, str(start_value),                             str(end_value)])                 dimensions_parameter.values = values     def __init__(self):         self.label = "OPeNDAP to NetCDF"         self.description = "The OPeNDAP to NetCDF tool will subset and " \             "download data from web-based servers which support the OPeNDAP " \             "protocol."                  self.canRunInBackground = False              def isLicensed(self):         return True;     def getParameterInfo(self):                  parameters = []         parameters.append(arcpy.Parameter(             displayName="Input OPeNDAP Dataset",             name="in_opendap_dataset",             datatype="GPString",             multiValue=False,             parameterType="Required",             direction="Input"))         parameters.append(arcpy.Parameter(             displayName="Variables",             name="variable",             datatype="GPString",             multiValue=True,             parameterType="Required",             direction="Input"))         parameters.append(arcpy.Parameter(             displayName="Output netCDF File",             name="out_netcdf_file",             datatype="DEFile",             multiValue=False,             parameterType="Required",             direction="Output"))         parameters.append(arcpy.Parameter(             displayName="Extent",             name="extent",             datatype="GPEnvelope",             multiValue=False,             parameterType="Optional",             direction="Input"))         parameters.append(arcpy.Parameter(             displayName="Dimensions",             name="dimension",             datatype="GPValueTable",             multiValue=True,             parameterType="Optional",             direction="Input"))         parameters[-1].columns = [             ["String", "Dimension"],             ["String", "Start Value"],             ["String", "End Value"]]         parameters.append(arcpy.Parameter(             displayName="Value Selection Method",             name="value_selection_method",             datatype="GPString",             multiValue=False,             parameterType="Optional",             direction="Input"))         parameters[-1].filter.list = ["BY_VALUE", "BY_INDEX"]         return parameters     def updateParameters(self,             parameters):                                    class_ = self.__class__         dataset_parameter = parameters[0]         variables_parameter = parameters[1]         output_file_parameter = parameters[2]         extent_parameter = parameters[3]         dimensions_parameter = parameters[4]         value_selection_method_parameter = parameters[5]         dataset = None                  try:             if not dataset_parameter is None:                 dataset = mds.netcdf.Dataset(dataset_parameter.valueAsText,                     filter_out_nd_coordinates=True)         except:             pass         for parameter in iter(parameters):             assert not (class_.parameter_must_be_initialized(parameter,                     dataset_parameter) and                 class_.parameter_must_be_updated(parameter,                     dataset_parameter)), parameter.name                  if class_.parameter_must_be_initialized(variables_parameter,                 dataset_parameter):             class_.initialize_variables_list(variables_parameter, dataset)         elif class_.parameter_must_be_updated(variables_parameter,                 dataset_parameter):             if not dataset is None:                 class_.update_variables_list(variables_parameter, dataset)                  if class_.parameter_must_be_initialized(output_file_parameter,                 dataset_parameter):             class_.initialize_output_file(output_file_parameter,                 dataset_parameter)                  if class_.parameter_must_be_initialized(extent_parameter,                 dataset_parameter):             class_.initialize_extent(extent_parameter, dataset,                 variables_parameter)                                    if class_.parameter_must_be_initialized(                 value_selection_method_parameter, dataset_parameter):             value_selection_method_parameter.value = "BY_VALUE"                  if class_.parameter_must_be_initialized(dimensions_parameter,                 dataset_parameter):             class_.initialize_dimensions(dimensions_parameter, dataset,                 value_selection_method_parameter.valueAsText,                 variables_parameter)     def updateMessages(self,             parameters):                           class_ = self.__class__         dataset_parameter = parameters[0]         variables_parameter = parameters[1]         output_file_parameter = parameters[2]         extent_parameter = parameters[3]         dimensions_parameter = parameters[4]         value_selection_method_parameter = parameters[5]         value_selection_method = mds.SELECT_BY_VALUE if \             value_selection_method_parameter.valueAsText == "BY_VALUE" else \                 mds.SELECT_BY_INDEX         dataset = None                  if not dataset_parameter.value is None:             try:                 dataset = mds.netcdf.Dataset(dataset_parameter.valueAsText,                     filter_out_nd_coordinates=True)             except RuntimeError, exception:                 if "No such file or directory" in str(exception) or \                         "Invalid argument" in str(exception):                     class_.set_error(dataset_parameter,                         mds.messages.INPUT_DATASET_DOES_NOT_RESOLVE_TO_FILENAME.format(                             dataset_parameter.valueAsText))                 elif "Malformed or inaccessible DAP DDS" in str(exception):                     class_.set_error(dataset_parameter,                         mds.messages.INPUT_DATASET_URL_MALFORMED.format(                             dataset_parameter.valueAsText))                 else:                     class_.set_error(dataset_parameter,                         mds.messages.INPUT_DATASET_GENERIC_ERROR.format(                             dataset_parameter.valueAsText, str(exception)))             except Exception, exception:                 class_.set_error(dataset_parameter,                     mds.messages.INPUT_DATASET_GENERIC_ERROR.format(                         dataset_parameter.valueAsText, str(exception)))                  if variables_parameter.values is not None:             if dataset is not None:                 variable_names_available = dataset.data_variable_names()                 variable_names_requested = mds.OrderedSet(                     variables_parameter.values)                 unknown_variable_names = variable_names_requested - \                     variable_names_available                 known_variable_names = variable_names_requested - \                     unknown_variable_names                 if unknown_variable_names:                     class_.set_warning(variables_parameter,                         mds.messages.VARIABLES_DO_NOT_EXIST.format(", ".join(                             unknown_variable_names), "Input OPeNDAP Dataset"))                 if len(known_variable_names) == 0:                     class_.set_error(variables_parameter,                         mds.messages.NONE_OF_VARIABLES_EXISTS)                 elif not class_.variables_are_compatible(                         known_variable_names, dataset):                     class_.set_error(variables_parameter,                         mds.messages.VARIABLES_MUST_SHARE_DIMENSIONS)                                                   dimension_names = set()                 for variable_name in known_variable_names:                     variable = dataset.variable(variable_name)                     for dimension_name in variable.dimensions:                         dimension_names.add(dimension_name)                 for dimension_name in dimension_names:                     if dimension_name in dataset.variable_names():                         if len(dataset.variable(dimension_name)[:].shape) > 1:                             class_.set_error(variables_parameter,                                 mds.messages.MULTIDIMENSIONAL_DIMENSIONS_NOT_SUPPORTED.format(                                     dimension_name))                             break                  if output_file_parameter.value is not None:             output_filename = output_file_parameter.valueAsText             if os.path.splitext(output_filename)[1] != ".nc":                 class_.set_error(output_file_parameter,                     mds.messages.OUTPUT_FILE_EXTENSION_MUST_BE_NC)                                    if dimensions_parameter.value is not None:             if dataset is not None:                 if variables_parameter.values is not None:                                                               dimension_names = set()                     for variable_name in variables_parameter.values:                         if variable_name in dataset.dataset.variables:                             variable = dataset.variable(variable_name)                             for dimension_name in variable.dimensions:                                 dimension_names.add(dimension_name)                     for dimension_record in dimensions_parameter.values:                                                                           dimension_name = dimension_record[0]                         if dimension_name not in dimension_names:                             class_.set_error(dimensions_parameter,                                 mds.messages.DIMENSION_NOT_PRESENT.format(                                     dimension_record[0]))                             break                         elif dimension_name in dataset.variable_names():                             if len(dataset.variable(dimension_name)[:].shape) > 1:                                 class_.set_error(dimensions_parameter,                                     mds.messages.MULTIDIMENSIONAL_DIMENSIONS_NOT_SUPPORTED.format(                                         dimension_name))                                 break                             if value_selection_method == mds.SELECT_BY_VALUE \                                     and dataset.convention.is_time_dimension_variable(                                         dimension_name):                                                                  _, start_value, end_value = dimension_record                                 try:                                     mds.date_time.from_iso_format(start_value)                                     mds.date_time.from_iso_format(end_value)                                 except ValueError:                                     class_.set_error(dimensions_parameter,                                         mds.messages.INVALID_DATE_TIME)                                     break                             elif dataset.convention.is_space_dimension_variable(                                     dimension_name):                                 class_.set_error(dimensions_parameter,                                     mds.messages.SKIPPING_SPATIAL_DIMENSION)                                 break     def execute(self,             parameters,             messages):         dataset_name = parameters[0].valueAsText         variable_names = mds.OrderedSet(parameters[1].values)         output_filename = parameters[2].valueAsText         extent = None         if parameters[3].value is not None:             extent = [float(value) for value in                 parameters[3].valueAsText.split(" ")]         dimension_records = parameters[4].values         value_selection_method = mds.SELECT_BY_VALUE if \             parameters[5].valueAsText == "BY_VALUE" else mds.SELECT_BY_INDEX         date_time_string = time.strftime("%m/%d/%Y %H:%M", time.localtime())         history_message = mds.messages.OPENDAP_TO_NETCDF_HISTORY.format(             date_time_string, dataset_name)         try:             dataset = mds.netcdf.Dataset(dataset_name,                 filter_out_nd_coordinates=True)                          known_variable_names = \                 variable_names & dataset.data_variable_names()             assert len(known_variable_names) > 0               unknown_variable_names = variable_names - known_variable_names             if unknown_variable_names:                 messages.addWarningMessage(                     mds.messages.VARIABLES_DO_NOT_EXIST.format(", ".join(                         unknown_variable_names), "Input OPeNDAP Dataset"))             mds.netcdf.copy(dataset, known_variable_names, output_filename,                 extent, dimension_records, value_selection_method,                 history_message)         except RuntimeError, exception:                          messages.addErrorMessage(str(exception))             raise arcpy.ExecuteError
8c55beda927bd7684fdf1705c7e1542deff64764 import re from module.plugins.internal.SimpleCrypter import SimpleCrypter, create_getInfo class UploadedToFolder(SimpleCrypter):     __name__    = "UploadedToFolder"     __type__    = "crypter"     __version__ = "0.47"     __status__  = "testing"     __pattern__ = r'https?://(?:www\.)?(uploaded|ul)\.(to|net)/(f|folder|list)/\w+'     __config__  = [("activated"            , "bool", "Activated"                                        , True),                    ("use_premium"          , "bool", "Use premium account if available"                 , True),                    ("use_subfolder"        , "bool", "Save package to subfolder"                        , True),                    ("subfolder_per_package", "bool", "Create a subfolder for each package"              , True),                    ("max_wait"             , "int" , "Reconnect if waiting time is greater than minutes", 10  )]     __description__ = """UploadedTo decrypter plugin"""     __license__     = "GPLv3"     __authors__     = [("stickell", "l.stickell@yahoo.it")]     NAME_PATTERN         = r'<title>(?P<N>.+?)<'     OFFLINE_PATTERN      = r'>Page not found'     TEMP_OFFLINE_PATTERN = r'<title>uploaded\.net - Maintenance'     LINK_PATTERN = r'<h2><a href="(.+?)"' getInfo = create_getInfo(UploadedToFolder)
6ce236fe03a09c64f2375a3f6bf1b431afc3a1ef """ Creating and updating scripts """ from world import world, setup_module, teardown_module import create_script_steps as script_create import create_execution_steps as execution_create class TestExecution(object):     def setup(self):         """             Debug information         """         print "\n-------------------\nTests in: %s\n" % __name__     def teardown(self):         """             Debug information         """         print "\nEnd of tests in: %s\n-------------------\n" % __name__     def test_scenario1(self):         """             Scenario: Successfully creating a whizzml script execution:                 Given I create a whizzml script from a excerpt of code "<source_code>"                 And I wait until the script is ready less than <time_1> secs                 And I create a whizzml script execution from an existing script                 And I wait until the execution is ready less than <time_2> secs                 And I update the execution with "<param>", "<param_value>"                 And I wait until the execution is ready less than <time_3> secs                 Then the script id is correct, the value of "<param>" is "<param_value>" and the result is "<result>"                 Examples:                 | source_code      | time_1  | time_2  | time_3  | param | param_value | result                 | (+ 1 1)          | 10      | 10      | 10      | name  | my execution | 2         """         print self.test_scenario1.__doc__         examples = [             ['(+ 1 1)', '10', '10', '10', 'name', 'my execution', 2]]         for example in examples:             print "\nTesting with:\n", example             script_create.i_create_a_script(self, example[0])             script_create.the_script_is_finished(self, example[1])             execution_create.i_create_an_execution(self)             execution_create.the_execution_is_finished(self, example[2])             execution_create.i_update_an_execution(self, example[4], example[5])             execution_create.the_execution_is_finished(self, example[3])             execution_create.the_execution_and_attributes(self, example[4], example[5], example[6])     def test_scenario2(self):         """             Scenario: Successfully creating a whizzml script execution from a list of scripts:                 Given I create a whizzml script from a excerpt of code "<source_code>"                 And I wait until the script is ready less than <time_1> secs                 And I create a whizzml script from a excerpt of code "<source_code>"                 And I wait until the script is ready less than <time_1> secs                 And I create a whizzml script execution from the last two scripts                 And I wait until the execution is ready less than <time_2> secs                 And I update the execution with "<param>", "<param_value>"                 And I wait until the execution is ready less than <time_3> secs                 Then the script ids are correct, the value of "<param>" is "<param_value>" and the result is "<result>"                 Examples:                 | source_code      | time_1  | time_2  | time_3  | param | param_value | result                 | (+ 1 1)          | 10      | 10      | 10      | name  | my execution | [2, 2]         """         print self.test_scenario2.__doc__         examples = [             ['(+ 1 1)', '10', '10', '10', 'name', 'my execution', [2, 2]]]         for example in examples:             print "\nTesting with:\n", example             script_create.i_create_a_script(self, example[0])             script_create.the_script_is_finished(self, example[1])             script_create.i_create_a_script(self, example[0])             script_create.the_script_is_finished(self, example[1])             execution_create.i_create_an_execution_from_list(self, 2)             execution_create.the_execution_is_finished(self, example[2])             execution_create.i_update_an_execution(self, example[4], example[5])             execution_create.the_execution_is_finished(self, example[3])             execution_create.the_execution_ids_and_attributes(self, 2, example[4], example[5], example[6])
72672e9d6615f34dbd73db3c48aecef966fe9a21 import os import re import sys import subprocess def RunCmdAndCheck(cmd, err_string, output_api, cwd=None):   results = []   p = subprocess.Popen(cmd, cwd=cwd,                        stdout=subprocess.PIPE,                        stderr=subprocess.PIPE)   (p_stdout, p_stderr) = p.communicate()   if p.returncode:     results.append(         output_api.PresubmitError(err_string,                                   long_text=p_stderr))   return results def RunUnittests(input_api, output_api):      results = []   files = input_api.LocalPaths()   generator_files = []   for filename in files:     name_parts = filename.split(os.sep)     if name_parts[0:2] == ['ppapi', 'generators']:       generator_files.append(filename)   if generator_files != []:     cmd = [ sys.executable, 'idl_tests.py']     ppapi_dir = input_api.PresubmitLocalPath()     results.extend(RunCmdAndCheck(cmd,                                   'PPAPI IDL unittests failed.',                                   output_api,                                   os.path.join(ppapi_dir, 'generators')))   return results RE_TODO = re.compile(r'\WTODO\W', flags=re.I) def CheckTODO(input_api, output_api):   files = input_api.LocalPaths()   todo = []   for filename in files:     name, ext = os.path.splitext(filename)     name_parts = name.split(os.sep)          if ext not in ['.h', '.idl']:       continue          if name_parts[0] != 'ppapi':       continue          if name_parts[1] not in ['api', 'c', 'cpp', 'utility']:       continue          if name_parts[2] in ['dev', 'private', 'trusted']:       continue     if name_parts[2] == 'extensions' and name_parts[3] == 'dev':       continue     filepath = os.path.join('..', filename)     if RE_TODO.search(open(filepath, 'rb').read()):       todo.append(filename)   if todo:     return [output_api.PresubmitError(         'TODOs found in stable public PPAPI files:',         long_text='\n'.join(todo))]   return [] RE_UNVERSIONED_PPB = re.compile(r'\bPPB_\w+_INTERFACE\b') def CheckUnversionedPPB(input_api, output_api):   files = input_api.LocalPaths()   todo = []   for filename in files:     name, ext = os.path.splitext(filename)     name_parts = name.split(os.sep)          if ext not in ['.cc']:       continue          if name_parts[0:2] != ['ppapi', 'cpp']:       continue          if name_parts[2] in ['dev', 'private']:       continue     filepath = os.path.join('..', filename)     if RE_UNVERSIONED_PPB.search(open(filepath, 'rb').read()):       todo.append(filename)   if todo:     return [output_api.PresubmitError(         'Unversioned PPB interface references found in PPAPI C++ wrappers:',         long_text='\n'.join(todo))]   return [] def CheckChange(input_api, output_api):   results = []   results.extend(RunUnittests(input_api, output_api))   results.extend(CheckTODO(input_api, output_api))   results.extend(CheckUnversionedPPB(input_api, output_api))      files = input_api.LocalPaths()   h_files = []   idl_files = []      for filename in files:     name, ext = os.path.splitext(filename)     name_parts = name.split(os.sep)     if name_parts[0:2] == ['ppapi', 'c'] and ext == '.h':       h_files.append('/'.join(name_parts[2:]))     if name_parts[0:2] == ['ppapi', 'api'] and ext == '.idl':       idl_files.append('/'.join(name_parts[2:]))      both = h_files + idl_files      if not both: return results   missing = []   for filename in idl_files:     if filename not in set(h_files):       missing.append('ppapi/api/%s.idl' % filename)         new_thunk_files = []   for filename in missing:     lines = input_api.RightHandSideLines(lambda f: f.LocalPath() == filename)     for line in lines:       if line[2].strip() == '[generate_thunk]':         new_thunk_files.append(filename)   for filename in new_thunk_files:     missing.remove(filename)   if missing:     results.append(         output_api.PresubmitPromptWarning(             'Missing PPAPI header, no change or skipped generation?',             long_text='\n  '.join(missing)))   missing_dev = []   missing_stable = []   missing_priv = []   for filename in h_files:     if filename not in set(idl_files):       name_parts = filename.split(os.sep)       if name_parts[-1] == 'pp_macros':                           lines = input_api.RightHandSideLines(             lambda f: f.LocalPath() == 'ppapi/c/%s.h' % filename)         releaseChanged = False         for line in lines:           if line[2].split()[:2] == ['             results.append(                 output_api.PresubmitPromptOrNotify(                     'PPAPI_RELEASE has changed', long_text=line[2]))             releaseChanged = True             break         if releaseChanged:           continue       if 'trusted' in name_parts:         missing_priv.append('  ppapi/c/%s.h' % filename)         continue       if 'private' in name_parts:         missing_priv.append('  ppapi/c/%s.h' % filename)         continue       if 'dev' in name_parts:         missing_dev.append('  ppapi/c/%s.h' % filename)         continue       missing_stable.append('  ppapi/c/%s.h' % filename)   if missing_priv:     results.append(         output_api.PresubmitPromptWarning(             'Missing PPAPI IDL for private interface, please generate IDL:',             long_text='\n'.join(missing_priv)))   if missing_dev:     results.append(         output_api.PresubmitPromptWarning(             'Missing PPAPI IDL for DEV, required before moving to stable:',             long_text='\n'.join(missing_dev)))   if missing_stable:     results.append(         output_api.PresubmitError(             'Missing PPAPI IDL for stable interface:',             long_text='\n'.join(missing_stable)))               ppapi_dir = input_api.PresubmitLocalPath()   cmd = [sys.executable, 'generator.py',          '--wnone', '--diff', '--test','--cgen', '--range=start,end']      cmd.append('--out=' + ','.join([name + '.idl' for name in both]))   cmd_results = RunCmdAndCheck(cmd,                                'PPAPI IDL Diff detected: Run the generator.',                                output_api,                                os.path.join(ppapi_dir, 'generators'))   if cmd_results:     results.extend(cmd_results)   return results def CheckChangeOnUpload(input_api, output_api):   return CheckChange(input_api, output_api) def CheckChangeOnCommit(input_api, output_api):   return CheckChange(input_api, output_api)
7be258ff70039365a183a4afaa9a47c9620eee30 import piw from pi import agent, domain, bundles, atom, action, utils from pi.logic.shortcuts import T from . import midi_device_version as version from .midi_device_plg_native import midi_device OUT_KEY=1 OUT_STRIP_1=2 OUT_STRIP_2=3 OUT_BREATH=4 OUT_PEDAL_1=5 OUT_PEDAL_2=6 OUT_PEDAL_3=7 OUT_PEDAL_4=8 PRESSURE=0 ROLL=1 YAW=2 BREATH=3 STRIP_1=4 STRIP_2=5 PEDAL_1=6 PEDAL_2=7 PEDAL_3=8 IN_MIDI=1 IN_LIGHT=2 OUT_MIDI=10 class Agent(agent.Agent):     def __init__(self, address, ordinal):                  agent.Agent.__init__(self, signature=version, names='ableton push', ordinal=ordinal)         self.domain = piw.clockdomain_ctl()         self.domain.set_source(piw.makestring('*',0))         self[1] = atom.Atom(names='outputs')         self[1][1] = bundles.Output(1,False,names='key output')         self[1][2] = bundles.Output(2,False,names='pressure output')         self[1][3] = bundles.Output(3,False,names='roll output')         self[1][4] = bundles.Output(4,False,names='yaw output')         self[1][5] = bundles.Output(1,False,names='breath output')         self[1][6] = bundles.Output(1,False,names='strip position output',ordinal=1)         self[1][7] = bundles.Output(2,False,names='absolute strip output',ordinal=1)         self[1][8] = bundles.Output(1,False,names='strip position output',ordinal=2)         self[1][9] = bundles.Output(2,False,names='absolute strip output',ordinal=2)         self[1][10] = bundles.Output(1,False,names='pedal output',ordinal=1)         self[1][11] = bundles.Output(1,False,names='pedal output',ordinal=2)         self[1][12] = bundles.Output(1,False,names='pedal output',ordinal=3)         self[1][13] = bundles.Output(1,False,names='pedal output',ordinal=4)   		         self.koutput = bundles.Splitter(self.domain,self[1][1],self[1][2],self[1][3],self[1][4])         self.kpoly = piw.polyctl(10,self.koutput.cookie(),False,5) 		         self.boutput = bundles.Splitter(self.domain,self[1][5]) 		         self.s1output = bundles.Splitter(self.domain,self[1][6],self[1][7])         self.s2output = bundles.Splitter(self.domain,self[1][8],self[1][9]) 		         self.poutput1 = bundles.Splitter(self.domain,self[1][10])         self.poutput2 = bundles.Splitter(self.domain,self[1][11])         self.poutput3 = bundles.Splitter(self.domain,self[1][12])         self.poutput4 = bundles.Splitter(self.domain,self[1][13])         self[5]=bundles.Output(OUT_MIDI,False,names="midi output")         self.midi_output = bundles.Splitter(self.domain, self[5])            		         self.output=piw.sclone()         self.output.set_filtered_output(OUT_KEY,self.kpoly.cookie(),piw.first_filter(OUT_KEY))         self.output.set_filtered_output(OUT_STRIP_1,self.s1output.cookie(),piw.first_filter(OUT_STRIP_1))         self.output.set_filtered_output(OUT_STRIP_2,self.s2output.cookie(),piw.first_filter(OUT_STRIP_2))         self.output.set_filtered_output(OUT_BREATH,self.boutput.cookie(),piw.first_filter(OUT_BREATH))          self.output.set_filtered_output(OUT_PEDAL_1,self.poutput1.cookie(),piw.first_filter(OUT_PEDAL_1))         self.output.set_filtered_output(OUT_PEDAL_2,self.poutput2.cookie(),piw.first_filter(OUT_PEDAL_2))         self.output.set_filtered_output(OUT_PEDAL_3,self.poutput3.cookie(),piw.first_filter(OUT_PEDAL_4))         self.output.set_filtered_output(OUT_PEDAL_4,self.poutput4.cookie(),piw.first_filter(OUT_PEDAL_4))         self.output.set_filtered_output(OUT_MIDI,self.midi_output.cookie(),piw.first_filter(OUT_MIDI))                  self.device = midi_device(self.domain, self.output.cookie())         self[1][14] = atom.Atom(names='controller output',domain=domain.Aniso(),init=self.controllerinit())                                 self.input = bundles.VectorInput(self.device.cookie(), self.domain, signals=(IN_MIDI,IN_LIGHT,))         self[2] = atom.Atom(domain=domain.Aniso(), policy=self.input.vector_policy(IN_MIDI,False), names="midi input")         self[4] = atom.Atom(domain=domain.Aniso(), protocols='revconnect', policy=self.input.vector_policy(IN_LIGHT,False,clocked=False), names="light input") 		         self.device.channel(1)         self.device.velocity_sample(4)         self.device.enable_notes(True)         self.device.enable_velocity(True)         self.device.enable_poly_at(PRESSURE,True)         self.device.set_data_freq(0)         self.device.enable_control_notes(True)         self.device.set_colour(1,87)          self.device.set_colour(2,4)          self.device.set_colour(3,124)          self.device.set_pb_map(STRIP_1,True)              def controllerinit(self):     	scale=piw.makestring('[0,1,2,3,4,5,6,7,8,9,10,11,12]',0)     	octave=piw.makefloat_bounded(9,-1,0,-1,0)         dict=utils.makedict({'columnlen':self.device.get_columnlen(),'columnoffset':self.device.get_columnoffset(),'courselen':self.device.get_courselen(),'courseoffset':self.device.get_courseoffset(),'octave':octave,'scale':scale},0)         return dict agent.main(Agent)
3d29b2370bcea633a8bd3473c8bbd1c14656d18c {     "name": "Pricelists information on Products",     'version': '8.0.1.0.0',     'category': 'Sales & Purchases',     'sequence': 14,     'author':  'ADHOC SA',     'website': 'www.adhoc.com.ar',     'license': 'AGPL-3',     'summary': '',     "description": """ Pricelists information on Products ================================== TODO: ----- * Get right domain to pricelist items * Enable button to pricelist items * Send by default product or product template if you create a item from the product pricelist items button     """,     "depends": [         "product",     ],     'external_dependencies': {     },     "data": [         'views/product_view.xml',         'views/pricelist_view.xml',     ],     'demo': [     ],     'test': [     ],     'installable': True,     'auto_install': False,     'application': False, }
a27c1eae6c530c2e7b15052c3a219edbfbe0ed0b import copy import logging from lxml import etree, html from openerp import SUPERUSER_ID, api, tools from openerp.addons.website.models import website from openerp.http import request from openerp.osv import osv, fields _logger = logging.getLogger(__name__) class view(osv.osv):     _name = "ir.ui.view"     _inherit = ["ir.ui.view", "website.seo.metadata"]     _columns = {         'page': fields.boolean("Whether this view is a web page template (complete)"),         'customize_show': fields.boolean("Show As Optional Inherit"),         'website_id': fields.many2one('website', ondelete='cascade', string="Website"),     }     _defaults = {         'page': False,         'customize_show': False,     }     def unlink(self, cr, uid, ids, context=None):         res = super(view, self).unlink(cr, uid, ids, context=context)         self.clear_caches()         return res     def _view_obj(self, cr, uid, view_id, context=None):         if isinstance(view_id, basestring):             if 'website_id' in context:                 domain = [('key', '=', view_id), '|', ('website_id', '=', False), ('website_id', '=', context.get('website_id'))]                 rec_id = self.search(cr, uid, domain, order='website_id', context=context)             else:                 rec_id = self.search(cr, uid, [('key', '=', view_id)], context=context)             if rec_id:                 return self.browse(cr, uid, rec_id, context=context)[0]             else:                 return self.pool['ir.model.data'].xmlid_to_object(                     cr, uid, view_id, raise_if_not_found=True, context=context)         elif isinstance(view_id, (int, long)):             return self.browse(cr, uid, view_id, context=context)                  return view_id               def _views_get(self, cr, uid, view_id, options=True, bundles=False, context=None, root=True):         """ For a given view ``view_id``, should return:         * the view itself         * all views inheriting from it, enabled or not           - but not the optional children of a non-enabled child         * all views called from it (via t-call)         """         try:             view = self._view_obj(cr, uid, view_id, context=context)         except ValueError:             _logger.warning("Could not find view object with view_id '%s'" % (view_id))                          return []         while root and view.inherit_id:             view = view.inherit_id         result = [view]         node = etree.fromstring(view.arch)         xpath = "//t[@t-call]"         if bundles:             xpath += "| //t[@t-call-assets]"         for child in node.xpath(xpath):             try:                 called_view = self._view_obj(cr, uid, child.get('t-call', child.get('t-call-assets')), context=context)             except ValueError:                 continue             if called_view not in result:                 result += self._views_get(cr, uid, called_view, options=options, bundles=bundles, context=context)         extensions = view.inherit_children_ids         if not options:                          extensions = (v for v in view.inherit_children_ids if v.active)                  for extension in sorted(extensions, key=lambda v: v.id):             for r in self._views_get(                     cr, uid, extension,                                          options=extension.active,                     context=context, root=False):                 if r not in result:                     result.append(r)         return result     @tools.ormcache_context('uid', 'xml_id', keys=('website_id',))     def get_view_id(self, cr, uid, xml_id, context=None):         if context and 'website_id' in context and not isinstance(xml_id, (int, long)):             domain = [('key', '=', xml_id), '|', ('website_id', '=', context['website_id']), ('website_id', '=', False)]             [view_id] = self.search(cr, uid, domain, order='website_id', limit=1, context=context) or [None]             if not view_id:                 raise ValueError('View %r in website %r not found' % (xml_id, context['website_id']))         else:             view_id = super(view, self).get_view_id(cr, uid, xml_id, context=context)         return view_id     @api.cr_uid_ids_context     def render(self, cr, uid, id_or_xml_id, values=None, engine='ir.qweb', context=None):         if request and getattr(request, 'website_enabled', False):             engine = 'ir.qweb'             if isinstance(id_or_xml_id, list):                 id_or_xml_id = id_or_xml_id[0]             qcontext = self._prepare_qcontext(cr, uid, context=context)                          if values:                 qcontext.update(values)                          if not qcontext.get('translatable'):                 if qcontext.get('editable'):                     context = dict(context, inherit_branding=True)                 elif request.registry['res.users'].has_group(cr, uid, 'base.group_website_publisher'):                     context = dict(context, inherit_branding_auto=True)             view_obj = request.website.get_template(id_or_xml_id)             if 'main_object' not in qcontext:                 qcontext['main_object'] = view_obj             values = qcontext         return super(view, self).render(cr, uid, id_or_xml_id, values=values, engine=engine, context=context)     def _prepare_qcontext(self, cr, uid, context=None):         if not context:             context = {}         company = self.pool['res.company'].browse(cr, SUPERUSER_ID, request.website.company_id.id, context=context)         editable = request.website.is_publisher()         translatable = editable and context.get('lang') != request.website.default_lang_code         editable = not translatable and editable         qcontext = dict(             context.copy(),             website=request.website,             url_for=website.url_for,             slug=website.slug,             res_company=company,             user_id=self.pool.get("res.users").browse(cr, uid, uid),             default_lang_code=request.website.default_lang_code,             languages=request.website.get_languages(),             translatable=translatable,             editable=editable,             menu_data=self.pool['ir.ui.menu'].load_menus_root(cr, uid, context=context) if request.website.is_user() else None,         )         return qcontext     def customize_template_get(self, cr, uid, key, full=False, bundles=False, context=None):         """ Get inherit view's informations of the template ``key``. By default, only         returns ``customize_show`` templates (which can be active or not), if         ``full=True`` returns inherit view's informations of the template ``key``.         ``bundles=True`` returns also the asset bundles         """         imd = self.pool['ir.model.data']         theme_view_id = imd.xmlid_to_res_id(cr, uid, 'website.theme')         user = self.pool['res.users'].browse(cr, uid, context=context)         user_groups = set(user.groups_id)         views = self._views_get(             cr, uid, key, bundles=bundles,             context=dict(context or {}, active_test=False))         done = set()         result = []         for v in views:             if not user_groups.issuperset(v.groups_id):                 continue             if full or (v.customize_show and v.inherit_id.id != theme_view_id):                 if v.inherit_id not in done:                     result.append({                         'name': v.inherit_id.name,                         'id': v.id,                         'key': v.key,                         'inherit_id': v.inherit_id.id,                         'header': True,                         'active': False                     })                     done.add(v.inherit_id)                 result.append({                     'name': v.name,                     'id': v.id,                     'key': v.key,                     'inherit_id': v.inherit_id.id,                     'header': False,                     'active': v.active,                 })         return result
bb261b7bacef76f4b2cbdd6760d155b0204fd649 from openerp.osv import orm from .postlogistics.web_service import PostlogisticsWebServiceShop class stock_picking(orm.Model):     _inherit = 'stock.picking'     def _generate_postlogistics_label(self, cr, uid, picking,                                       webservice_class=None,                                       tracking_ids=None,                                       context=None):         """ Generate post label using shop label """         if webservice_class is None:             webservice_class = PostlogisticsWebServiceShop         return super(stock_picking, self)._generate_postlogistics_label(             cr, uid, picking,             webservice_class=webservice_class,             tracking_ids=tracking_ids,             context=context)
05ec4441e63cbc36857ff3c81dc4b3189d883368 from openerp import models, fields, api class ResPartner(models.Model):     _inherit = 'res.partner'     tax_receipt_option = fields.Selection([         ('none', 'None'),         ('each', 'For Each Donation'),         ('annual', 'Annual Tax Receipt'),         ], string='Tax Receipt Option')     @api.model     def _commercial_fields(self):         res = super(ResPartner, self)._commercial_fields()         res.append('tax_receipt_option')         return res
4d1ee2d19395d5c884e835a8ed2459fc7492cf07 from . import hr_department
0d6d1cacb58c66134913a11f0d7023347bb57cd7 from openerp.osv import fields, osv class AccountPaymentConfig(osv.TransientModel):     _inherit = 'account.config.settings'     _columns = {         'module_payment_transfer': fields.boolean(             'Wire Transfer',             help='-It installs the module payment_transfer.'),         'module_payment_paypal': fields.boolean(             'Paypal',             help='-It installs the module payment_paypal.'),         'module_payment_ogone': fields.boolean(             'Ogone',             help='-It installs the module payment_ogone.'),         'module_payment_adyen': fields.boolean(             'Adyen',             help='-It installs the module payment_adyen.'),         'module_payment_buckaroo': fields.boolean(             'Buckaroo',             help='-It installs the module payment_buckaroo.'),         'module_payment_authorize': fields.dummy(             'Authorize.Net',             help='-It installs the module payment_authorize.'),     }     _defaults = {         'module_payment_transfer': True     }
382f99a6c8612976bc2753053a5ee87870e8671c from django.shortcuts import render, get_object_or_404, redirect from django.contrib.auth.decorators import login_required from dissertation.models.adviser import Adviser from dissertation.models.dissertation_role import DissertationRole from base import models as mdl from dissertation.forms import AdviserForm, ManagerAdviserForm, ManagerAddAdviserForm from django.contrib.auth.decorators import user_passes_test from django.db import IntegrityError from django.db.models import Q def is_manager(user):     person = mdl.person.find_by_user(user)     adviser = Adviser.find_by_person(person)     return adviser.type == 'MGR' def is_teacher(user):     person = mdl.person.find_by_user(user)     adviser = Adviser.find_by_person(person)     return adviser.type == 'PRF' @login_required @user_passes_test(is_teacher) def informations(request):     person = mdl.person.find_by_user(request.user)     try:         adviser = Adviser(person=person, available_by_email=False, available_by_phone=False, available_at_office=False)         adviser.save()     except IntegrityError:         adviser = Adviser.find_by_person(person)     return render(request, "informations.html", {'adviser': adviser}) @login_required @user_passes_test(is_teacher) def informations_detail_stats(request):     person = mdl.person.find_by_user(request.user)     try:         adviser = Adviser(person=person, available_by_email=False, available_by_phone=False, available_at_office=False)         adviser.save()     except IntegrityError:         adviser = Adviser.find_by_person(person)     queryset = DissertationRole.objects.all()     count_advisers = queryset.filter(Q(adviser=adviser) & Q(dissertation__active=True)).count()     count_advisers_pro = queryset.filter(         Q(adviser=adviser) & Q(status='PROMOTEUR') &         Q(dissertation__active=True)).exclude(Q(dissertation__status='DRAFT') |                                               Q(dissertation__status='ENDED') |                                               Q(dissertation__status='DEFENDED')).count()     count_advisers_pro_request = queryset.filter(         Q(adviser=adviser) & Q(status='PROMOTEUR') &         Q(dissertation__status='DIR_SUBMIT') & Q(dissertation__active=True)).count()     advisers_copro = queryset.filter(         Q(adviser=adviser) & Q(status='CO_PROMOTEUR') &         Q(dissertation__active=True)).exclude(         Q(dissertation__status='DRAFT') | Q(dissertation__status='ENDED') |         Q(dissertation__status='DEFENDED'))     count_advisers_copro = advisers_copro.count()     tab_offer_count_copro = {}          for dissertaion_role_copro in advisers_copro:         if dissertaion_role_copro.dissertation.offer_year_start.offer.title in tab_offer_count_copro:             tab_offer_count_copro[dissertaion_role_copro.dissertation.offer_year_start.offer.title] = \                 tab_offer_count_copro[str(dissertaion_role_copro.dissertation.offer_year_start.offer.title)] + 1         else:             tab_offer_count_copro[dissertaion_role_copro.dissertation.offer_year_start.offer.title] = 1     advisers_reader = queryset.filter(Q(adviser=adviser) &                                       Q(status='READER') & Q(dissertation__active=True)).exclude(         Q(dissertation__status='DRAFT') |         Q(dissertation__status='ENDED') | Q(dissertation__status='DEFENDED'))     count_advisers_reader = advisers_reader.count()     tab_offer_count_read = {}     for dissertaion_role_read in advisers_reader:         if dissertaion_role_read.dissertation.offer_year_start.offer.title in tab_offer_count_read:             tab_offer_count_read[dissertaion_role_read.dissertation.offer_year_start.offer.title] = \                 tab_offer_count_read[str(dissertaion_role_read.dissertation.offer_year_start.offer.title)] + 1         else:             tab_offer_count_read[dissertaion_role_read.dissertation.offer_year_start.offer.title] = 1     advisers_pro = queryset.filter(Q(adviser=adviser) &                                    Q(status='PROMOTEUR') &                                    Q(dissertation__active=True)).exclude(Q(dissertation__status='DRAFT') |                                                                          Q(dissertation__status='ENDED') |                                                                          Q(dissertation__status='DEFENDED'))     tab_offer_count_pro = {}     for dissertaion_role_pro in advisers_pro:         if dissertaion_role_pro.dissertation.offer_year_start.offer.title in tab_offer_count_pro:             tab_offer_count_pro[dissertaion_role_pro.dissertation.offer_year_start.offer.title] = \                 tab_offer_count_pro[str(dissertaion_role_pro.dissertation.offer_year_start.offer.title)] + 1         else:             tab_offer_count_pro[dissertaion_role_pro.dissertation.offer_year_start.offer.title] = 1     return render(request, 'informations_detail_stats.html',                   {'adviser': adviser, 'count_advisers': count_advisers, 'count_advisers_copro': count_advisers_copro,                    'count_advisers_pro': count_advisers_pro, 'count_advisers_reader': count_advisers_reader,                    'count_advisers_pro_request': count_advisers_pro_request,                    'tab_offer_count_pro': tab_offer_count_pro, 'tab_offer_count_read': tab_offer_count_read,                    'tab_offer_count_copro': tab_offer_count_copro}) @login_required @user_passes_test(is_teacher) def informations_edit(request):     person = mdl.person.find_by_user(request.user)     adviser = Adviser.find_by_person(person)     if request.method == "POST":         form = AdviserForm(request.POST, instance=adviser)         if form.is_valid():             adviser = form.save(commit=False)             adviser.save()             return redirect('informations')     else:         form = AdviserForm(instance=adviser)     return render(request, "informations_edit.html", {'form': form, 'person': person}) @login_required @user_passes_test(is_manager) def manager_informations(request):     advisers = Adviser.objects.filter(type='PRF').order_by('person__last_name', 'person__first_name')     return render(request, 'manager_informations_list.html', {'advisers': advisers}) @login_required @user_passes_test(is_manager) def manager_informations_add(request):     if request.method == "POST":         form = ManagerAddAdviserForm(request.POST)         if form.is_valid():             form.save()             return redirect('manager_informations')     else:         form = ManagerAddAdviserForm(initial={'type': "PRF"})     return render(request, 'manager_informations_add.html', {'form': form}) @login_required @user_passes_test(is_manager) def manager_informations_detail(request, pk):     adviser = get_object_or_404(Adviser, pk=pk)     return render(request, 'manager_informations_detail.html', {'adviser': adviser}) @login_required @user_passes_test(is_manager) def manager_informations_edit(request, pk):     adviser = get_object_or_404(Adviser, pk=pk)     if request.method == "POST":         form = ManagerAdviserForm(request.POST, instance=adviser)         if form.is_valid():             adviser = form.save(commit=False)             adviser.save()             return redirect('manager_informations_detail', pk=adviser.pk)     else:         form = ManagerAdviserForm(instance=adviser)     return render(request, "manager_informations_edit.html", {'adviser': adviser, 'form': form}) @login_required @user_passes_test(is_manager) def manager_informations_search(request):     advisers = Adviser.search(terms=request.GET['search'])     return render(request, "manager_informations_list.html", {'advisers': advisers}) @login_required @user_passes_test(is_manager) def manager_informations_list_request(request):     queryset = DissertationRole.objects.all()     advisers_need_request = queryset.filter(Q(status='PROMOTEUR') &                                             Q(dissertation__status='DIR_SUBMIT') &                                             Q(dissertation__active=True))     advisers_need_request = advisers_need_request.order_by('adviser')     return render(request, "manager_informations_list_request.html", {'advisers_need_request': advisers_need_request}) @login_required @user_passes_test(is_manager) def manager_informations_detail_list(request, pk):     adviser = get_object_or_404(Adviser, pk=pk)     queryset = DissertationRole.objects.all()     adviser_list_dissertations = queryset.filter(Q(status='PROMOTEUR') &                                                  Q(adviser__pk=pk) &                                                  Q(dissertation__active=True)).exclude(                                                         Q(dissertation__status='DRAFT'))     adviser_list_dissertations = adviser_list_dissertations.order_by('dissertation__status')     adviser_list_dissertations_copro = queryset.filter(Q(status='CO_PROMOTEUR') &                                                        Q(adviser__pk=pk) &                                                        Q(dissertation__active=True)).exclude(                                                         Q(dissertation__status='DRAFT'))     adviser_list_dissertations_copro = adviser_list_dissertations_copro.order_by('dissertation__status')     adviser_list_dissertations_reader = queryset.filter(Q(status='READER') &                                                         Q(adviser__pk=pk) &                                                         Q(dissertation__active=True)).exclude(                                                         Q(dissertation__status='DRAFT'))     adviser_list_dissertations_reader = adviser_list_dissertations_reader.order_by('dissertation__status')     return render(request, "manager_informations_detail_list.html",                   {'adviser': adviser,                    'adviser_list_dissertations': adviser_list_dissertations,                    'adviser_list_dissertations_copro': adviser_list_dissertations_copro,                    'adviser_list_dissertations_reader': adviser_list_dissertations_reader,                    }) @login_required @user_passes_test(is_manager) def manager_informations_detail_stats(request, pk):     adviser = get_object_or_404(Adviser, pk=pk)     queryset = DissertationRole.objects.all()     count_advisers = queryset.filter(Q(adviser=adviser) & Q(dissertation__active=True)).count()     count_advisers_pro = queryset.filter(         Q(adviser=adviser) & Q(status='PROMOTEUR') &         Q(dissertation__active=True)).exclude(Q(dissertation__status='DRAFT') |                                               Q(dissertation__status='ENDED') |                                               Q(dissertation__status='DEFENDED')).count()     count_advisers_pro_request = queryset.filter(         Q(adviser=adviser) & Q(status='PROMOTEUR') &         Q(dissertation__status='DIR_SUBMIT') & Q(dissertation__active=True)).count()     advisers_copro = queryset.filter(         Q(adviser=adviser) & Q(status='CO_PROMOTEUR') &         Q(dissertation__active=True)).exclude(         Q(dissertation__status='DRAFT') | Q(dissertation__status='ENDED') |         Q(dissertation__status='DEFENDED'))     count_advisers_copro = advisers_copro.count()     tab_offer_count_copro = {}          for dissertaion_role_copro in advisers_copro:         if dissertaion_role_copro.dissertation.offer_year_start.offer.title in tab_offer_count_copro:             tab_offer_count_copro[dissertaion_role_copro.dissertation.offer_year_start.offer.title] = \                 tab_offer_count_copro[str(dissertaion_role_copro.dissertation.offer_year_start.offer.title)] + 1         else:             tab_offer_count_copro[dissertaion_role_copro.dissertation.offer_year_start.offer.title] = 1     advisers_reader = queryset.filter(Q(adviser=adviser) &                                       Q(status='READER') & Q(dissertation__active=True)).exclude(         Q(dissertation__status='DRAFT') |         Q(dissertation__status='ENDED') | Q(dissertation__status='DEFENDED'))     count_advisers_reader = advisers_reader.count()     tab_offer_count_read = {}     for dissertaion_role_read in advisers_reader:         if dissertaion_role_read.dissertation.offer_year_start.offer.title in tab_offer_count_read:             tab_offer_count_read[dissertaion_role_read.dissertation.offer_year_start.offer.title] = \                 tab_offer_count_read[str(dissertaion_role_read.dissertation.offer_year_start.offer.title)] + 1         else:             tab_offer_count_read[dissertaion_role_read.dissertation.offer_year_start.offer.title] = 1     advisers_pro = queryset.filter(Q(adviser=adviser) &                                    Q(status='PROMOTEUR') &                                    Q(dissertation__active=True)).exclude(Q(dissertation__status='DRAFT') |                                                                          Q(dissertation__status='ENDED') |                                                                          Q(dissertation__status='DEFENDED'))     tab_offer_count_pro = {}     for dissertaion_role_pro in advisers_pro:         if dissertaion_role_pro.dissertation.offer_year_start.offer.title in tab_offer_count_pro:             tab_offer_count_pro[dissertaion_role_pro.dissertation.offer_year_start.offer.title] = \                 tab_offer_count_pro[str(dissertaion_role_pro.dissertation.offer_year_start.offer.title)] + 1         else:             tab_offer_count_pro[dissertaion_role_pro.dissertation.offer_year_start.offer.title] = 1     return render(request, 'manager_informations_detail_stats.html',                   {'adviser': adviser, 'count_advisers': count_advisers, 'count_advisers_copro': count_advisers_copro,                    'count_advisers_pro': count_advisers_pro, 'count_advisers_reader': count_advisers_reader,                    'count_advisers_pro_request': count_advisers_pro_request,                    'tab_offer_count_pro': tab_offer_count_pro, 'tab_offer_count_read': tab_offer_count_read,                    'tab_offer_count_copro': tab_offer_count_copro})
6f7d59c65fd28701d99eb06c2ef9fd547d2c50d7 """ Demo platform that has two fake binary sensors. For more details about this platform, please refer to the documentation https://home-assistant.io/components/demo/ """ from homeassistant.components.binary_sensor import BinarySensorDevice def setup_platform(hass, config, add_devices, discovery_info=None):     """Setup the Demo binary sensor platform."""     add_devices([         DemoBinarySensor('Basement Floor Wet', False, 'moisture'),         DemoBinarySensor('Movement Backyard', True, 'motion'),     ]) class DemoBinarySensor(BinarySensorDevice):     """A Demo binary sensor."""     def __init__(self, name, state, sensor_class):         """Initialize the demo sensor."""         self._name = name         self._state = state         self._sensor_type = sensor_class     @property     def sensor_class(self):         """Return the class of this sensor."""         return self._sensor_type     @property     def should_poll(self):         """No polling needed for a demo binary sensor."""         return False     @property     def name(self):         """Return the name of the binary sensor."""         return self._name     @property     def is_on(self):         """Return true if the binary sensor is on."""         return self._state
8b11a8c7ec1ac56682539f1c241dadc89652bf40 from django.conf.urls import url from . import views urlpatterns = [               url(r'^$', views.index, name='index'),     url(r'^news/$', views.news, name='news'),     url(r'^2016/$', views.special_case_2016),     url(r'^latest/$', views.latest),     url(r'^category/(?P<categoryslug>.*)/$', views.category, name='category' ),     url(r'^(?P<postslug>.*)/$', views.detail, name='detail' ),                         url(r'^([0-9]{4})/$', views.year_archive),     url(r'^([0-9]{4})/([0-9]{2})/$', views.month_archive),          url(r'^(?P<year>[0-9]{4})/$', views.year_archive),     url(r'^(?P<year>[0-9]{4})/(?P<month>[0-9]{2})/$', views.month_archive),      ]
572573820d398b8c4b1f376d33fa6273b6c1e99a from v8cffi.exceptions import (     V8Error,     V8JSError,     V8MemoryError,     V8UnknownError) __all__ = [     'V8Error',     'V8JSError',     'V8MemoryError',     'V8UnknownError']
88c98d95019e8c8b97cc332fdeaff1da2472cc3f """Sciter's platform independent graphics interface. Incomplete. """ import enum from ctypes import * from sciter.capi.sctypes import SCFN, UINT, BOOL HGFX = c_void_p HIMG = c_void_p HPATH = c_void_p HTEXT = c_void_p class GRAPHIN_RESULT(enum.IntEnum):     """Result value for Sciter Graphics functions."""     GRAPHIN_PANIC = -1     GRAPHIN_OK = 0     GRAPHIN_BAD_PARAM = 1     GRAPHIN_FAILURE = 2     GRAPHIN_NOTSUPPORTED = 3 imageCreate = SCFN(GRAPHIN_RESULT, POINTER(HIMG), UINT, UINT, BOOL) class SciterGraphicsAPI(Structure):     """Sciter Graphics ABI."""     _fields_ = [         ("imageCreate", imageCreate),     ] LPSciterGraphicsAPI = POINTER(SciterGraphicsAPI)
d3d7e72195a4ddb263663996d7db4b329ad7c56b from __future__ import absolute_import, unicode_literals import mock from django.test import TestCase from logical.tests import factory as factory_logical from ..models import DatabaseInfra from . import factory from drivers.fake import FakeDriver from django.core.cache import cache import logging LOG = logging.getLogger(__name__) class DatabaseInfraTestCase(TestCase):     def setUp(self):                  cache.clear()     def test_best_for_without_plan_and_environment_options_returns_None(self):         plan = factory.PlanFactory()         environment = plan.environments.all()[0]         self.assertIsNone(             DatabaseInfra.best_for(plan=plan, environment=environment, name="test"))     def test_best_for_with_only_one_datainfra_per_plan_and_environment(self):         plan = factory.PlanFactory()         environment = plan.environments.all()[0]         datainfra = factory.DatabaseInfraFactory(             plan=plan, environment=environment)         instance = factory.InstanceFactory(             address="127.0.0.1", port=27017, databaseinfra=datainfra)         self.assertEqual(datainfra, DatabaseInfra.best_for(             plan=plan, environment=environment, name="test"))     def test_best_for_with_only_two_datainfra_per_plan_and_environment_returns_rounding_robin_them(self):         plan = factory.PlanFactory()         environment = plan.environments.all()[0]         datainfra1 = factory.DatabaseInfraFactory(             plan=plan, environment=environment, capacity=10)         instance1 = factory.InstanceFactory(             address="127.0.0.1", port=27017, databaseinfra=datainfra1)         datainfra2 = factory.DatabaseInfraFactory(             plan=plan, environment=environment, capacity=10)         instance2 = factory.InstanceFactory(             address="127.0.0.2", port=27017, databaseinfra=datainfra2)         for i in range(10):             should_choose = (datainfra1, datainfra2)[i % 2]             choosed = DatabaseInfra.best_for(                 plan=plan, environment=environment, name="test")             self.assertEqual(should_choose, choosed)             database = factory_logical.DatabaseFactory(databaseinfra=choosed)             self.assertEqual(choosed, database.databaseinfra)     def test_check_instances_status_is_alive(self):         plan = factory.PlanFactory()         environment = plan.environments.all()[0]         datainfra1 = factory.DatabaseInfraFactory(             plan=plan, environment=environment, capacity=10)         instance1 = factory.InstanceFactory(             address="127.0.0.1", port=27017, databaseinfra=datainfra1, status=1)         instance2 = factory.InstanceFactory(             address="127.0.0.2", port=27017, databaseinfra=datainfra1, status=1)         self.assertEquals(             datainfra1.check_instances_status(), DatabaseInfra.ALIVE)     def test_check_instances_status_is_dead(self):         plan = factory.PlanFactory()         environment = plan.environments.all()[0]         datainfra1 = factory.DatabaseInfraFactory(             plan=plan, environment=environment, capacity=10)         instance1 = factory.InstanceFactory(             address="127.0.0.1", port=27017, databaseinfra=datainfra1, status=0)         instance2 = factory.InstanceFactory(             address="127.0.0.2", port=27017, databaseinfra=datainfra1, status=0)         self.assertEquals(             datainfra1.check_instances_status(), DatabaseInfra.DEAD)     def test_check_instances_status_is_alert(self):         plan = factory.PlanFactory()         environment = plan.environments.all()[0]         datainfra1 = factory.DatabaseInfraFactory(             plan=plan, environment=environment, capacity=10)         instance1 = factory.InstanceFactory(             address="127.0.0.1", port=27017, databaseinfra=datainfra1, status=1)         instance2 = factory.InstanceFactory(             address="127.0.0.2", port=27017, databaseinfra=datainfra1, status=0)         self.assertEquals(             datainfra1.check_instances_status(), DatabaseInfra.ALERT)     def test_best_for_with_only_over_capacity_datainfra_returns_None(self):         """tests database infra capacity"""         NUMBER_OF_DATABASES_TO_TEST = 4         plan = factory.PlanFactory()         environment = plan.environments.all()[0]         datainfra = factory.DatabaseInfraFactory(             plan=plan, environment=environment, capacity=NUMBER_OF_DATABASES_TO_TEST)         instance = factory.InstanceFactory(             address="127.0.0.1", port=27017, databaseinfra=datainfra)         for i in range(NUMBER_OF_DATABASES_TO_TEST):             self.assertEqual(datainfra, DatabaseInfra.best_for(                 plan=plan, environment=environment, name="test"))             factory_logical.DatabaseFactory(databaseinfra=datainfra)         self.assertIsNone(             DatabaseInfra.best_for(plan=plan, environment=environment, name="test"))     @mock.patch.object(FakeDriver, 'info')     def test_get_info_use_caching(self, info):         info.return_value = 'hahaha'         datainfra = factory.DatabaseInfraFactory()         self.assertIsNotNone(datainfra.get_info())                  datainfra = DatabaseInfra.objects.get(pk=datainfra.pk)         self.assertIsNotNone(datainfra.get_info())         info.assert_called_once_with()     @mock.patch.object(FakeDriver, 'info')     def test_get_info_accept_force_refresh(self, info):         info.return_value = 'hahaha'         datainfra = factory.DatabaseInfraFactory()         self.assertIsNotNone(datainfra.get_info())                  datainfra = DatabaseInfra.objects.get(pk=datainfra.pk)         self.assertIsNotNone(datainfra.get_info(force_refresh=True))         self.assertEqual(2, info.call_count)
da4437cdcceded81fbf30e8f73b3a4fc584c91d5 """ slate provides a convenient interface to PDFMiner[1]. Intializing a slate.PDF object will provide you with the text from the source file as a list of pages. So, a five page PDF file will have a range of 0-4.     >>> with open('example.pdf', 'rb') as f:     ...    PDF(f)      ...     [..., ..., ..., ...] Beware of page numbers. slate.PDF objects start at 0.     >>> with open('example.pdf', 'rb') as f:     ...    doc = PDF(f)     ...     >>> "Hello, I'm page three." in doc[2]     True Blank pages are empty strings:     >>> doc[1]     '' If you would prefer to access the entire text as a single string of text, you can call the text method, which will remove unnecessary whitespace, e.g. '  \n  \x0c' => ' '.     >>> "Hello" in doc.text()     True     >>> '\x0c' in doc.text()     False Passwords are supported. Use them as the second argument of your intialization. Currently, UTF-8 encoding is hard-coded. If you would like to access more advanced features, you should take a look at the PDFMiner API[2].     >>> with open('protected.pdf', 'rb') as f:     ...    PDF(f, 'a')[0].strip()     'Chamber of secrets.'   [1] http://www.unixuser.org/~euske/python/pdfminer/index.html   [2] http://www.unixuser.org/~euske/python/pdfminer/programming.html """ from .classes import PDF
72bbf69717f6ff0dbdb11eedaebd34660beab5c9 """ Created on 5 fvr. 2014 @author: inso """ import datetime import logging import asyncio from ..tools.decorators import asyncify, once_at_a_time, cancel_once_task from PyQt5.QtCore import QAbstractTableModel, Qt, QVariant, QSortFilterProxyModel, \     QDateTime, QLocale, QModelIndex from PyQt5.QtGui import QFont, QColor class CertsFilterProxyModel(QSortFilterProxyModel):     def __init__(self, ts_from, ts_to, parent=None):         super().__init__(parent)         self.app = None         self.ts_from = ts_from         self.ts_to = ts_to     @property     def account(self):         return self.app.current_account     def set_period(self, ts_from, ts_to):         """         Filter table by given timestamps         """         logging.debug("Filtering from {0} to {1}".format(             datetime.datetime.fromtimestamp(ts_from).isoformat(' '),             datetime.datetime.fromtimestamp(ts_to).isoformat(' '))         )         self.ts_from = ts_from         self.ts_to = ts_to         self.modelReset.emit()     def filterAcceptsRow(self, sourceRow, sourceParent):         def in_period(date_ts):             return date_ts in range(self.ts_from, self.ts_to)         source_model = self.sourceModel()         date_col = source_model.columns_types.index('date')         source_index = source_model.index(sourceRow, date_col)         date = source_model.data(source_index, Qt.DisplayRole)         return in_period(date)     @property     def community(self):         return self.sourceModel().community     def columnCount(self, parent):         return self.sourceModel().columnCount(None) - 5     def setSourceModel(self, sourceModel):         self.app = sourceModel.app         super().setSourceModel(sourceModel)     def lessThan(self, left, right):         """         Sort table by given column number.         """         source_model = self.sourceModel()         left_data = source_model.data(left, Qt.DisplayRole)         right_data = source_model.data(right, Qt.DisplayRole)         if left_data == "":             return self.sortOrder() == Qt.DescendingOrder         elif right_data == "":             return self.sortOrder() == Qt.AscendingOrder         return (left_data < right_data)     def data(self, index, role):         source_index = self.mapToSource(index)         model = self.sourceModel()         source_data = model.data(source_index, role)         state_col = model.columns_types.index('state')         state_index = model.index(source_index.row(), state_col)         state_data = model.data(state_index, Qt.DisplayRole)         if role == Qt.DisplayRole:             if source_index.column() == model.columns_types.index('uid'):                 return source_data             if source_index.column() == model.columns_types.index('date'):                 return QLocale.toString(                     QLocale(),                     QDateTime.fromTime_t(source_data).date(),                     QLocale.dateFormat(QLocale(), QLocale.ShortFormat)                 )             if source_index.column() == model.columns_types.index('payment') or \                     source_index.column() == model.columns_types.index('deposit'):                 return source_data         if role == Qt.FontRole:             font = QFont()             return font         if role == Qt.ForegroundRole:             if state_data == TransferState.REFUSED:                 return QColor(Qt.red)             elif state_data == TransferState.TO_SEND:                 return QColor(Qt.blue)         if role == Qt.TextAlignmentRole:             if source_index.column() == self.sourceModel().columns_types.index('date'):                 return Qt.AlignCenter         if role == Qt.ToolTipRole:             if source_index.column() == self.sourceModel().columns_types.index('date'):                 return QDateTime.fromTime_t(source_data).toString(Qt.SystemLocaleLongDate)             return None         return source_data class HistoryTableModel(QAbstractTableModel):     """     A Qt abstract item model to display communities in a tree     """     def __init__(self, app, account, community, parent=None):         """         Constructor         """         super().__init__(parent)         self.app = app         self.account = account         self.community = community         self.transfers_data = []         self.refresh_certs()         self._max_confirmations = 0         self.columns_types = (             'date',             'uid',             'state',             'pubkey',             'block_number'         )         self.column_headers = (             self.tr('Date'),             self.tr('UID/Public key'),             'State',             'Pubkey',             'Block Number'         )     def change_account(self, account):         cancel_once_task(self, self.refresh_certs)         self.account = account     def change_community(self, community):         cancel_once_task(self, self.refresh_certs)         self.community = community     def certifications(self):         if self.account:             return self.account.certifications(self.community)         else:             return []     @once_at_a_time     @asyncify     async def refresh_certs(self):         self.beginResetModel()         self.transfers_data = []         self.endResetModel()     def max_confirmations(self):         return self._max_confirmations     def rowCount(self, parent):         return len(self.transfers_data)     def columnCount(self, parent):         return len(self.columns_types)     def headerData(self, section, orientation, role):         if self.account and self.community:             if role == Qt.DisplayRole:                 return self.column_headers[section]     def data(self, index, role):         row = index.row()         col = index.column()         if not index.isValid():             return QVariant()         if role == Qt.DisplayRole:             return self.transfers_data[row][col]         if role == Qt.ToolTipRole:             return self.transfers_data[row][col]     def flags(self, index):         return Qt.ItemIsSelectable | Qt.ItemIsEnabled
bb6237a5f5caf23afa0e60ba4c3f9e9a01d8922d import os import sipconfig import platform from exceptions import KeyError try:     mgchome = os.environ["MGC_AMS_HOME"] except KeyError:     raise Exception("MGC_AMS_HOME not set, please load Mentor AMS tools setup") build_file = "jwdb.sbf" config = sipconfig.Configuration() os.system(" ".join([config.sip_bin, "-c", ".", "-b", build_file, "jwdb.sip"])) makefile = sipconfig.SIPModuleMakefile(config, build_file) makefile.extra_include_dirs = [os.path.join(mgchome, 'include')] if platform.machine() == 'x86_64':     makefile.extra_lib_dirs = [os.path.join(mgchome, 'aol', 'lib')]     makefile.extra_libs = ["eldogwl_64"] else:     makefile.extra_lib_dirs = [os.path.join(mgchome, 'ixl', 'lib')]     makefile.extra_libs = ["gwl"] makefile.generate()
7c028e6cc1942cca32b2be6a072a4c4484305801 import requests from lxml.html import fromstring from database import MongoDb as Database class IbgeTracker():     def __init__(self):         self.url_ufs = 'http://www.ibge.gov.br/home/geociencias' + \                        '/areaterritorial/principal.shtm'         self.url_cidades = 'http://www.ibge.gov.br/home/geociencias' + \                            '/areaterritorial/area.php?nome=%'     def _request(self, url):         response = requests.post(url)         response.raise_for_status()         return response.text     def _get_info_ufs(self, siglas):         texto = self._request(self.url_ufs)         html = fromstring(texto)         seletorcss_linhas = "div#miolo_interno > table > tr"         linhas = html.cssselect(seletorcss_linhas)         try:             linhas.pop(0)           except IndexError:             pass         infos = []         for linha in linhas:             seletorcss_celulas = "td"             celulas = linha.cssselect(seletorcss_celulas)             codigo_ibge = celulas[0].text_content()             if codigo_ibge in siglas:                 sigla = siglas[codigo_ibge]                 infos.append({                     'sigla': sigla,                     'codigo_ibge': codigo_ibge,                     'nome': celulas[1].text_content().strip(' (*)'),                     'area_km2': celulas[2].text_content()                 })                                    return infos     def _get_info_cidades(self):         texto = self._request(self.url_cidades)         html = fromstring(texto)         seletorcss_linhas = "div#miolo_interno > table > tr"         linhas = html.cssselect(seletorcss_linhas)         try:             linhas.pop(0)           except IndexError:             pass         infos = []         for linha in linhas:             seletorcss_celulas = "td"             celulas = linha.cssselect(seletorcss_celulas)             infos.append({                 'codigo_ibge_uf': celulas[0].text_content(),                 'sigla_uf': celulas[1].text_content(),                 'codigo_ibge': celulas[2].text_content(),                 'nome': celulas[3].text_content(),                 'area_km2': celulas[4].text_content()             })         return infos     def _track_ufs(self, db, siglas):         infos = self._get_info_ufs(siglas)         for info in infos:             db.insert_or_update_uf(info)     def _track_cidades(self, db):         infos = self._get_info_cidades()         siglas = {}         for info in infos:             codigo_ibge_uf = info['codigo_ibge_uf']             sigla_uf = info['sigla_uf']             nome = info['nome']             if codigo_ibge_uf not in siglas:                 siglas[codigo_ibge_uf] = sigla_uf                                                                 info['sigla_uf_nome_cidade'] = '%s_%s' % (sigla_uf, nome)             db.insert_or_update_cidade(info)         return siglas     def track(self, db):         """         Atualiza as bases internas do mongo         com os dados mais recentes do IBGE         referente a ufs e cidades         """         siglas = self._track_cidades(db)                           self._track_ufs(db, siglas) def _standalone():     db = Database()     ibge = IbgeTracker()     ibge.track(db) if __name__ == "__main__":     _standalone()
951ba0fed45299b2e5994dd655aa2c4535170216 ''' GraphTree.py -  ====================================================== :Author: Andreas Heger :Release: $Id$ :Date: |today| :Tags: Python Code ---- ''' import sys from Graph import * import Phylolib FORMAT = """ """ def SortAByB( a, b):     c = map( lambda x,y: (y, x), a, b)     c.sort()     return map(lambda x: x[1], c) class GraphTree( Graph ):     def __init__(self, tree, order = None, info = None, colours = None ):         Graph.__init__(self,FORMAT)         num_leaves = tree.getNumLeaves()                  if not order: order = tree.getNodesDepthFirstFinish()                  last_height = 0.0         vertical_order = 1                  order = list(order)         if colours: colours = list(colours)         if info: info = list(info)         heights = []         for index in order:             heights.append(tree.getHeight( index ))                      order = SortAByB(order, heights)         for index in order:                          if colours:                 colour = colours[index]             else:                 colour = None                              if last_height < tree.getHeight(index):                 vertical_order += 2                 last_height = tree.getHeight(index)             if info:                 info1 = info[index]             else:                 info1 = None                              n = Node( str(index),                       str(index),                       vertical_order = vertical_order,                       info1 = info1,                       colour = colour                       )                              self.AddNode( n )                          if index >= num_leaves:                 e = Edge( str(tree.getLeftChild(index)), str(index) )                 self.AddEdge(e)                 e = Edge( str(tree.getRightChild(index)), str(index) )                 self.AddEdge(e)                       class GraphTreeExplicit( Graph ):     def __init__(self,                  tree,                  info = None,                  colours = None ):         Graph.__init__(self,FORMAT)         num_leaves = tree.getNumLeaves()                  order = tree.getNodesDepthFirstFinish()                  coords = [0] * num_leaves * 2         self.mStepSizeX = 100          self.mStepSizeY = 100          self.mBaseLineY = 0                  current_x = 0         current_y = self.mBaseLineY + self.mStepSizeY         for node in order:                          if node < num_leaves:                 coords[node] = (current_x, self.mBaseLineY)                  current_x += self.mStepSizeX             else:                 left_child  = tree.getLeftChild(node)                 right_child = tree.getRightChild(node)                                  coords[node] = ((coords[left_child][0] + coords[right_child][0]) / 2, current_y)                 current_y += self.mStepSizeY         for index in order:                          if colours:                 colour = colours[index]             else:                 colour = None                              if info:                 info1 = info[index]             else:                 info1 = None                              n = Node( str(index),                       str(index),                       location = coords[index],                       info1 = info1,                       colour = colour                       )                              self.AddNode( n )                          if index >= num_leaves:                 e = Edge( str(tree.getLeftChild(index)), str(index) )                 self.AddEdge(e)                 e = Edge( str(tree.getRightChild(index)), str(index) )                 self.AddEdge(e)                           if __name__ == "__main__":     t = Phylolib.makeTree(5)     t.joinNodes( 0, 1, 1.0, 1.0)     t.joinNodes( 2, 3, 1.0, 1.0)     t.joinNodes( 4, 5, 1.0, 1.0)     t.joinNodes( 6, 7, 1.0, 1.0)          g = GraphTree( t,                    order = t.getNodesDepthFirstFinish(),                    colours=["blue", "green", "red", "yellow", "black", "blue", "green", "red", "yellow"],                    )          g.Write( sys.stdout )     
3e666b92616f3eff6e8bbf52dc6986e338ef3d7d from terrabot.util.streamer import Streamer class Packet39Parser(object):     def parse(self, world, player, data, ev_man):         streamer = Streamer(data)         streamer.next_byte()          item_index = streamer.next_short()         world.item_owner_index[item_index] = 255
5975c0b901bccb148a69436dca8d729e6c4418c3 """ Handles 'cfy ssh' """ import os import re import platform import subprocess from distutils import spawn from cloudify_cli import utils from cloudify_cli.logger import get_logger from cloudify_cli.ssh import run_command_on_manager from cloudify_cli.exceptions import CloudifyCliError def _open_interactive_shell(host_string, command=''):     """Used as fabric's open_shell=True doesn't work well.     (Disfigures coloring and such...)     """     ssh_key_path = os.path.expanduser(utils.get_management_key())     cmd = ['ssh', '-t', host_string, '-i', ssh_key_path]     if command:         cmd.append(command)     subprocess.call(cmd) def _verify_tmux_exists_on_manager(host_string):     try:         run_command_on_manager('which tmux', host_string=host_string)     except:         raise CloudifyCliError(             'tmux executable not found on manager {0}.\n'             'Please verify that tmux is installed and in PATH before '             'attempting to use shared SSH sessions.\n'             'You can run `cfy ssh -c "sudo yum install tmux -y"` to try and '             'install tmux on the manager.'.format(                 host_string.split('@')[1])) def _send_keys(logger, command, sid, host_string):     logger.debug('Sending "{0}" to session...'.format(command))     run_command_on_manager(         'tmux send-keys -t {0} \'{1}\' C-m'.format(sid, command),         host_string=host_string) def _validate_env(ssh_command, host_session, sid, list_sessions):     if not isinstance(ssh_command, str):         raise CloudifyCliError('ssh_command should be a string.')     if not isinstance(host_session, bool):         raise CloudifyCliError('host_session should be a boolean.')     if not isinstance(sid, str):         raise CloudifyCliError('sid should be a str.')     if not isinstance(list_sessions, bool):         raise CloudifyCliError('list_sessions should be a boolean.')     ssh_path = spawn.find_executable('ssh')     if not ssh_path:         raise CloudifyCliError(             "ssh not found. Possible reasons:\n"             "1) You don't have ssh installed (try installing OpenSSH)\n"             "2) Your PATH variable is not configured correctly\n"             "3) You are running this command with Sudo which can manipulate "             "environment variables for security reasons")     if not ssh_path and platform.system() == 'Windows':         raise CloudifyCliError(             "ssh.exe not found. Are you sure you have it installed? "             "As an alternative, you can use PuTTY to ssh into the manager. "             "Do not forget to convert your private key from OpenSSH format to "             "PuTTY's format using PuTTYGen.")     if any([host_session and sid,             host_session and list_sessions,             sid and list_sessions]):         raise CloudifyCliError(             'Choose one of --host, --list-sessions, --sid arguments.') def _join_session(logger, sid, host_string):     logger.info('Attempting to join session...')     if sid not in _get_sessions_list(logger, host_string):         logger.error('Session {0} does not exist'.format(sid))         return     _open_interactive_shell(         command='tmux attach -t {0}'.format(sid),         host_string=host_string) def _get_all_sessions(logger, host_string):     logger.info('Retrieving list of existing sessions...')     try:                  output = run_command_on_manager(             'tmux list-sessions',             host_string=host_string)     except:         return None     return output def _get_sessions_list(logger, host_string):     return re.findall(         r'ssh_session_\w{6}',         _get_all_sessions(logger, host_string)) def ssh(ssh_command, host_session, sid, list_sessions):     """Connects to a running manager via SSH.     `host_session` starts a tmux session (e.g. tmux new -s     "ssh_session_vi120m") after which a command for a client is printed     in the tmux session for the host to send to the client     (i.e. cfy ssh --sid ssh_session_vi120m).     When starting a new session, the host creates an alias for "exit"     so that when a client connects and exits, it will run "tmux detach"     instead and not kill the session.     When the host exits the tmux session, a command will be executed     to kill the session.     Passing an `ssh_command` will simply execute it on the manager while     omitting a command will connect to an interactive shell.     """     _validate_env(ssh_command, host_session, sid, list_sessions)     host_string = utils.build_manager_host_string()     if host_session or sid or list_sessions:         _verify_tmux_exists_on_manager(host_string)     logger = get_logger()     logger.info('Connecting to {0}...'.format(host_string))     if host_session:         sid = 'ssh_session_' + utils.generate_random_string()         logger.info('Creating session {0}...'.format(sid))         try:             run_command_on_manager(                 'tmux new -d -A -s {0}'.format(sid),                 host_string=host_string)             logger.info('Preparing environment...')             _send_keys(logger, 'alias exit="tmux detach"; clear', sid,                        host_string=host_string)             _send_keys(logger, '                        'to join the session.'.format(sid), sid,                        host_string=host_string)             _join_session(logger, sid, host_string)         except Exception as ex:             logger.error('Failed to create session ({0})'.format(ex))         logger.info('Killing session {0}...'.format(sid))         try:             run_command_on_manager(                 'tmux kill-session -t {0}'.format(sid),                 host_string=host_string)         except Exception as ex:             logger.warn('Failed to kill session ({0})'.format(ex))     elif sid:         _join_session(logger, sid, host_string)     elif list_sessions:         sessions = _get_all_sessions(logger, host_string)         if sessions:             logger.info('Available Sessions are:\n{0}'.format(sessions.stdout))         else:             logger.info('No sessions are available')     else:         if ssh_command:             logger.info('Executing command {0}...'.format(ssh_command))             run_command_on_manager(                 ssh_command,                 host_string=host_string,                 force_output=True)         else:             _open_interactive_shell(host_string=host_string)
6db388e1f2c15398c7ba6a273e31f3d535df3e25 import numpy as _np from six.moves import range from pyemma._base.estimator import Estimator as _Estimator from pyemma._base.progress import ProgressReporter as _ProgressReporter from pyemma.thermo import MEMM as _MEMM from pyemma.msm import MSM as _MSM from pyemma.util import types as _types from pyemma.util.units import TimeUnit as _TimeUnit from pyemma.thermo.estimators._callback import _ConvergenceProgressIndicatorCallBack from pyemma.thermo.estimators._callback import _IterationProgressIndicatorCallBack from thermotools import tram as _tram from thermotools import tram_direct as _tram_direct from thermotools import mbar as _mbar from thermotools import mbar_direct as _mbar_direct from thermotools import util as _util from thermotools import cset as _cset from msmtools.estimation import largest_connected_set as _largest_connected_set import warnings as _warnings class EmptyState(RuntimeWarning):     pass class TRAM(_Estimator, _MEMM, _ProgressReporter):     r"""Transition(-based) Reweighting Analysis Method     Parameters     ----------     lag : int         Integer lag time at which transitions are counted.     count_mode : str, optional, default='sliding'         mode to obtain count matrices from discrete trajectories. Should be         one of:         * 'sliding' : A trajectory of length T will have :math:`T-\tau` counts at time indexes               .. math::                  (0 \rightarrow \tau), (1 \rightarrow \tau+1), ..., (T-\tau-1 \rightarrow T-1)         * 'sample' : A trajectory of length T will have :math:`T/\tau` counts           at time indexes               .. math::                     (0 \rightarrow \tau), (\tau \rightarrow 2 \tau), ..., ((T/\tau-1) \tau \rightarrow T)         Currently only 'sliding' is supported.     maxiter : int, optional, default=10000         The maximum number of self-consistent iterations before the estimator exits unsuccessfully.     maxerr : float, optional, default=1E-15         Convergence criterion based on the maximal free energy change in a self-consistent         iteration step.     save_convergence_info : int, optional, default=0         Every save_convergence_info iteration steps, store the actual increment         and the actual loglikelihood; 0 means no storage.     dt_traj : str, optional, default='1 step'         Description of the physical time corresponding to the lag. May be used by analysis         algorithms such as plotting tools to pretty-print the axes. By default '1 step', i.e.         there is no physical time unit.  Specify by a number, whitespace and unit. Permitted         units are (* is an arbitrary string):         |  'fs',   'femtosecond*'         |  'ps',   'picosecond*'         |  'ns',   'nanosecond*'         |  'us',   'microsecond*'         |  'ms',   'millisecond*'         |  's',    'second*'     connectivity : str, optional, default='summed_count_matrix'         One of 'summed_count_matrix', 'strong_in_every_ensemble',         'neighbors', 'post_hoc_RE' or 'BAR_variance'.         Defines what should be considered a connected set in the joint space         of conformations and thermodynamic ensembles.         For details see thermotools.cset.compute_csets_TRAM.     nn : int, optional, default=None         Only needed if connectivity='neighbors'         See thermotools.cset.compute_csets_TRAM.     connectivity_factor : float, optional, default=1.0         Only needed if connectivity='post_hoc_RE' or 'BAR_variance'. Weakens the connectivity         requirement, see thermotools.cset.compute_csets_TRAM.     direct_space : bool, optional, default=False         Whether to perform the self-consitent iteration with Boltzmann factors         (direct space) or free energies (log-space). When analyzing data from         multi-temperature simulations, direct-space is not recommended.     N_dtram_accelerations : int, optional, default=0         Convergence of TRAM can be speeded up by interleaving the updates         in the self-consitent iteration with a dTRAM-like update step.         N_dtram_accelerations says how many times the dTRAM-like update         step should be applied in every iteration of the TRAM equations.         Currently this is only effective if direct_space=True.     init : str, optional, default=None         Use a specific initialization for self-consistent iteration:         | None:    use a hard-coded guess for free energies and Lagrangian multipliers         | 'mbar':  perform a short MBAR estimate to initialize the free energies     init_maxiter : int, optional, default=5000         The maximum number of self-consistent iterations during the initialization.     init_maxerr : float, optional, default=1.0E-8         Convergence criterion for the initialization.     References     ----------     .. [1] Wu, H. et al 2016         in press     """     def __init__(         self, lag, count_mode='sliding',         connectivity='summed_count_matrix',         ground_state=None,         maxiter=10000, maxerr=1.0E-15, save_convergence_info=0, dt_traj='1 step',         nn=None, connectivity_factor=1.0, direct_space=False, N_dtram_accelerations=0,         callback=None,         init='mbar', init_maxiter=5000, init_maxerr=1.0E-8):         self.lag = lag         assert count_mode == 'sliding', 'Currently the only implemented count_mode is \'sliding\''         self.count_mode = count_mode         self.connectivity = connectivity         self.nn = nn         self.connectivity_factor = connectivity_factor         self.dt_traj = dt_traj         self.timestep_traj = _TimeUnit(dt_traj)         self.ground_state = ground_state         self.maxiter = maxiter         self.maxerr = maxerr         self.direct_space = direct_space         self.N_dtram_accelerations = N_dtram_accelerations         self.callback = callback         self.save_convergence_info = save_convergence_info         assert init in (None, 'mbar'), 'Currently only None and \'mbar\' are supported'         self.init = init         self.init_maxiter = init_maxiter         self.init_maxerr = init_maxerr         self.active_set = None         self.biased_conf_energies = None         self.mbar_therm_energies = None         self.log_lagrangian_mult = None         self.loglikelihoods = None     def _estimate(self, X):         """         Parameters         ----------         X : tuple of (ttrajs, dtrajs, btrajs)             Simulation trajectories. ttrajs contain the indices of the thermodynamic state, dtrajs             contains the indices of the configurational states and btrajs contain the biases.         ttrajs : list of numpy.ndarray(X_i, dtype=int)             Every elements is a trajectory (time series). ttrajs[i][t] is the index of the             thermodynamic state visited in trajectory i at time step t.         dtrajs : list of numpy.ndarray(X_i, dtype=int)             dtrajs[i][t] is the index of the configurational state (Markov state) visited in             trajectory i at time step t.         btrajs : list of numpy.ndarray((X_i, T), dtype=numpy.float64)             For every simulation frame seen in trajectory i and time step t, btrajs[i][t,k] is the             bias energy of that frame evaluated in the k'th thermodynamic state (i.e. at the k'th             Umbrella/Hamiltonian/temperature).         """         ttrajs, dtrajs_full, btrajs = X                  assert len(ttrajs) == len(dtrajs_full) == len(btrajs)         for t in ttrajs:             _types.assert_array(t, ndim=1, kind='i')         for d in dtrajs_full:             _types.assert_array(d, ndim=1, kind='i')         for b in btrajs:             _types.assert_array(b, ndim=2, kind='f')                  self.nstates_full = max(_np.max(d) for d in dtrajs_full)+1         self.nthermo = max(_np.max(t) for t in ttrajs)+1                  for t, d, b, in zip(ttrajs, dtrajs_full, btrajs):             assert t.shape[0] == d.shape[0] == b.shape[0]             assert b.shape[1] == self.nthermo                  ttrajs = [_np.require(t, dtype=_np.intc, requirements='C') for t in ttrajs]         dtrajs_full = [_np.require(d, dtype=_np.intc, requirements='C') for d in dtrajs_full]         btrajs = [_np.require(b, dtype=_np.float64, requirements='C') for b in btrajs]                  state_counts_full = _util.state_counts(ttrajs, dtrajs_full)         count_matrices_full = _util.count_matrices(ttrajs, dtrajs_full,             self.lag, sliding=self.count_mode, sparse_return=False, nstates=self.nstates_full)         self.therm_state_counts_full = state_counts_full.sum(axis=1)         self.csets, pcset = _cset.compute_csets_TRAM(             self.connectivity, state_counts_full, count_matrices_full,             ttrajs=ttrajs, dtrajs=dtrajs_full, bias_trajs=btrajs,             nn=self.nn, factor=self.connectivity_factor,             callback=_IterationProgressIndicatorCallBack(self, 'finding connected set', 'cset'))         self.active_set = pcset                  for k in range(self.nthermo):             if len(self.csets[k]) == 0:                 _warnings.warn(                     'Thermodynamic state %d' % k \                     + ' contains no samples after reducing to the connected set.', EmptyState)                  self.state_counts, self.count_matrices, self.dtrajs, _  = _cset.restrict_to_csets(             self.csets,             state_counts=state_counts_full, count_matrices=count_matrices_full,             ttrajs=ttrajs, dtrajs=dtrajs_full)                  assert _np.all(self.state_counts >= _np.maximum(self.count_matrices.sum(axis=1), \             self.count_matrices.sum(axis=2)))         assert _np.all(_np.sum(             [_np.bincount(d[d>=0], minlength=self.nstates_full) for d in self.dtrajs],             axis=0) == self.state_counts.sum(axis=0))         assert _np.all(_np.sum(             [_np.bincount(t[d>=0], minlength=self.nthermo) for t, d in zip(ttrajs, self.dtrajs)],             axis=0) == self.state_counts.sum(axis=1))                  for k in range(self.state_counts.shape[0]):             if self.count_matrices[k, :, :].sum() == 0:                 _warnings.warn(                     'Thermodynamic state %d' % k \                     + 'contains no transitions after reducing to the connected set.', EmptyState)         if self.init == 'mbar' and self.biased_conf_energies is None:             if self.direct_space:                 mbar = _mbar_direct             else:                 mbar = _mbar             self.mbar_therm_energies, self.mbar_unbiased_conf_energies, \                 self.mbar_biased_conf_energies, _ = mbar.estimate(                     state_counts_full.sum(axis=1), btrajs, dtrajs_full,                     maxiter=self.init_maxiter, maxerr=self.init_maxerr,                     callback=_ConvergenceProgressIndicatorCallBack(                         self, 'MBAR init.', self.init_maxiter, self.init_maxerr),                     n_conf_states=self.nstates_full)             self._progress_force_finish(stage='MBAR init.', description='MBAR init.')             self.biased_conf_energies = self.mbar_biased_conf_energies.copy()                  if self.direct_space:             tram = _tram_direct         else:             tram = _tram                                    self.biased_conf_energies, conf_energies, self.therm_energies, self.log_lagrangian_mult, \             self.increments, self.loglikelihoods = tram.estimate(                 self.count_matrices, self.state_counts, btrajs, self.dtrajs,                 maxiter=self.maxiter, maxerr=self.maxerr,                 biased_conf_energies=self.biased_conf_energies,                 log_lagrangian_mult=self.log_lagrangian_mult,                 save_convergence_info=self.save_convergence_info,                 callback=_ConvergenceProgressIndicatorCallBack(                     self, 'TRAM', self.maxiter, self.maxerr),                 N_dtram_accelerations=self.N_dtram_accelerations)         self._progress_force_finish(stage='TRAM', description='TRAM')         self.btrajs = btrajs                  fmsms = [_np.ascontiguousarray((             _tram.estimate_transition_matrix(                 self.log_lagrangian_mult, self.biased_conf_energies, self.count_matrices, None,                 K)[self.active_set, :])[:, self.active_set]) for K in range(self.nthermo)]         self.model_active_set = [_largest_connected_set(msm, directed=False) for msm in fmsms]         fmsms = [_np.ascontiguousarray(             (msm[lcc, :])[:, lcc]) for msm, lcc in zip(fmsms, self.model_active_set)]         models = [_MSM(msm, dt_model=self.timestep_traj.get_scaled(self.lag)) for msm in fmsms]                  self.set_model_params(             models=models, f_therm=self.therm_energies, f=conf_energies[self.active_set].copy())         return self     def log_likelihood(self):         r"""         Returns the value of the log-likelihood of the converged TRAM estimate.         """                  return _tram.log_likelihood_lower_bound(             self.log_lagrangian_mult, self.biased_conf_energies,             self.count_matrices, self.btrajs, self.dtrajs, self.state_counts,             None, None, None, None, None)     def pointwise_free_energies(self, therm_state=None):         r"""         Computes the pointwise free energies :math:`-\log(\mu^k(x))` for all points x.         :math:`\mu^k(x)` is the optimal estimate of the Boltzmann distribution         of the k'th ensemble defined on the set of all samples.         Parameters         ----------         therm_state : int or None, default=None             Selects the thermodynamic state k for which to compute the             pointwise free energies.             None selects the "unbiased" state which is defined by having             zero bias energy.         Returns         -------         mu_k : list of numpy.ndarray(X_i, dtype=numpy.float64)              list of the same layout as dtrajs (or ttrajs). mu_k[i][t]              contains the pointwise free energy of the frame seen in              trajectory i and time step t.              Frames that are not in the connected sets get assiged an              infinite pointwise free energy.         """         assert self.therm_energies is not None, \             'MEMM has to be estimate()\'d before pointwise free energies can be calculated.'         if therm_state is not None:             assert therm_state<=self.nthermo         mu = [_np.zeros(d.shape[0], dtype=_np.float64) for d in self.dtrajs]         _tram.get_pointwise_unbiased_free_energies(             therm_state,             self.log_lagrangian_mult, self.biased_conf_energies,             self.therm_energies, self.count_matrices,             self.btrajs, self.dtrajs,             self.state_counts, None, None, mu)         return mu     def mbar_pointwise_free_energies(self, therm_state=None):         assert self.mbar_therm_energies is not None, \             'MEMM has to be estimate()\'d with init=\'mbar\' before pointwise free energies can be calculated.'         if therm_state is not None:             assert therm_state<=self.nthermo         mu = [_np.zeros(d.shape[0], dtype=_np.float64) for d in self.dtrajs]         _mbar.get_pointwise_unbiased_free_energies(therm_state,             _np.log(self.therm_state_counts_full), self.btrajs,              self.mbar_therm_energies, None, mu)         return mu
