from pyspark import SparkContext
from pyspark.sql import SQLContext
from pyspark import SparkConf
from pyspark.sql import SparkSession
import csv
import re

conf = (SparkConf().setMaster("spark://ec2-34-239-35-112.compute-1.amazonaws.c$
sc = SparkContext(conf = conf)
sqlContext = SQLContext(sc)
#gdelt_bucket = "s3n://samplecontentrepos/results-20190124-124325_s3.csv"
gdelt_bucket = "s3n://samplecontentrepos/test_000000000000.csv"
df = sqlContext.read.csv(gdelt_bucket, header = True)
rdd = sqlContext.read.csv(gdelt_bucket, header = True).rdd
pattern = r',(?=")'
github =  rdd.map(lambda x: re.split(pattern, x)
#print rdd.take(20)
github.show()
#df = sqlContext.read \
#    .format('com.databricks.spark.csv') \
#    .options(header='true') \
#    .options(delimiter=",") \
#    .load(gdelt_bucket)
